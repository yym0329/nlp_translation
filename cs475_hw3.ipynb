{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnXsTDPaTmAo"
      },
      "source": [
        "# CS475 MLNLP Homework 3: RNN & Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TAG9s6JTmAt"
      },
      "source": [
        "## Submission Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rVuWZwgTmAv"
      },
      "source": [
        "This homework is based on a few number of sources, some of them are pytorch tutorial about Seq2Seq network and other AI/ML based courses in KAIST. In order to do this homework, you should be able to code using pytorch base function (such as `nn.Module`) and understand the equation of networks that we will implement here.\n",
        "\n",
        "**How to submit**\n",
        "- Fill out TODO blocks, DO NOT modify other parts of the skeleton code. Sample of TODO blocks:\n",
        "```\n",
        "##################\n",
        "# TODO: Implement the forward function of LSTMCell\n",
        "##################\n",
        "\n",
        "# Your implementation code\n",
        "pass\n",
        "\n",
        "##################\n",
        "# END OF YOUR CODE\n",
        "##################\n",
        "```\n",
        "- Submit one file: hw3_{student_ID}.ipynb to KLMS e.g.hw3_20243150.ipynb\n",
        "- Late submission policy: After the submission deadline, you will immediately lose 20% of the score, another 20% after 24 hours later, and so on. Submission after 72 hours (3 days) will not be counted. However, you can use late days for this assignment, for which you have to send an email to inform the TAs. See course syllabus website Late Policy section.\n",
        "\n",
        "**Note**\n",
        "- Make a copy of this .ipynb file. Do not directly edit this file.\n",
        "- You cannot import any additional libraries other than provided in the skeleton code\n",
        "- You cannot use some of the pytorch built-in classes such as `nn.Linear`, `nn.RNN`, `nn.LSTM`, `nn.LSTMCell`, etc. which correspond to things that we will implement here. Use your own implementation that you will be doing here instead.\n",
        "- Some of the pytorch built-in classes are still allowed such as `nn.Parameter`, `nn.Embedding`, `nn.LayerNorm`, etc. Activation functions, optimizers, loss function, are also allowed.\n",
        "- For all weights $W$, initialize the weights with a **random tensor** and **multiply it by 0.1 scale**\n",
        "- For all bias $b$, must be **initialized to 0 tensor**\n",
        "- Check whether your whole cells work well by restarting runtime code and running all before the submission.\n",
        "- TA will look into the implemented functions, their validity and give corresponding score to each TODO problem.\n",
        "- Ask questions through Slack so that you can share information with other students.\n",
        "- TA in charge: Faiz Ghifari Haznitrama (haznitrama@kaist.ac.kr)\n",
        "\n",
        "**In this programming assignment, you will**\n",
        "- Understand the implementation of RNN and Transformer in pytorch, from low to high level\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1MvW8RWTmAy"
      },
      "source": [
        "## Changelog\n",
        "\n",
        "**v1.0.0**\n",
        "- Release HW3\n",
        "\n",
        "**v1.1.0**\n",
        "- Fix potential bug when decoding text data by forcing encoding `utf-8`\n",
        "- Move `set_random_seeds` forward to under Data Processing section\n",
        "- Fix bug on text preprocessing by removing `contains_only_valid_characters` function when filtering the data, because it made the total filtered data under 1000\n",
        "- Fix the output documentation of `RNNTranslator` `forward` function\n",
        "- Change the implementation of `compute_accuracy` function by removing two lines of code that shift pred tokens and target seq\n",
        "- - We remove this part since every student implementation in various parts related to this can be different\n",
        "- Add assertion to check whether `predicted_tokens` and `target_seq` has the same shape or not in `compute_accuracy`\n",
        "- Add more details about BOS_TOKEN and PAD_TOKEN that also must be ignored when implemmenting `evaluate_model`\n",
        "- Add **important notes** regarding `batch_size` in Set Hyperparameters, Build Vocab, Load Dataset section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZK_VJMPTmA0"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vi-uQG7KTmA2"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import requests\n",
        "import unicodedata\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztjvo1KITmA6"
      },
      "source": [
        "### GPU Availability Check & Set Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw07CeQuTmA7"
      },
      "source": [
        "Since we are going to train RNN and Transformer, make sure you work on this homework with CUDA capable machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtnfIb2LTmA8",
        "outputId": "8d69205e-49ca-45d5-f26f-16fb177ec7a2"
      },
      "outputs": [],
      "source": [
        "class CUDANotAvailableError(Exception):\n",
        "    \"\"\"Custom exception for when CUDA is required but not available.\"\"\"\n",
        "    def __init__(self, message=\"CUDA is required but not available on this system\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "def check_cuda_availability():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise CUDANotAvailableError()\n",
        "    else:\n",
        "        print(\"CUDA is available. Proceeding with GPU acceleration.\")\n",
        "        return torch.device(\"cuda:0\")\n",
        "\n",
        "def set_random_seeds(seed=42):\n",
        "    \"\"\"\n",
        "    Set random seeds for reproducibility across different libraries\n",
        "\n",
        "    Args:\n",
        "        seed (int): The random seed to use. Default is 42.\n",
        "    \"\"\"\n",
        "    # Set Python's random seed\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Set NumPy's random seed\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set PyTorch's random seed\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Set CUDA's random seed if available\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
        "\n",
        "      # These settings ensure deterministic behavior for CUDA\n",
        "      torch.backends.cudnn.deterministic = True\n",
        "      torch.backends.cudnn.benchmark = False\n",
        "    else:\n",
        "      raise CUDANotAvailableError()\n",
        "\n",
        "    # Set environment variable for any other libraries\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "DEVICE = check_cuda_availability()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGm2HoITmA9"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b38Zc4knTmA-"
      },
      "source": [
        "We are going to use the pytorch tutorial NMT English-France dataset. The function below should download the data and store it on the root directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rf40iSSRTmA_"
      },
      "outputs": [],
      "source": [
        "def download_extract_read(url, zip_filename, txt_filename):\n",
        "\n",
        "    if not os.path.isfile(zip_filename):\n",
        "      # Download the zip file\n",
        "      response = requests.get(url)\n",
        "      if response.status_code != 200:\n",
        "          raise Exception(f\"Failed to download file. Status code: {response.status_code}\")\n",
        "\n",
        "      # Save the zip file\n",
        "      with open(zip_filename, 'wb') as file:\n",
        "          file.write(response.content)\n",
        "      print(f\"Zip file saved as {zip_filename}\")\n",
        "\n",
        "    if not os.path.isfile(txt_filename):\n",
        "      # Extract the txt file from the zip\n",
        "      with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "          zip_ref.extract(txt_filename)\n",
        "      print(f\"Extracted {txt_filename} from the zip file\")\n",
        "\n",
        "    # Read the contents of the txt file\n",
        "    with open(txt_filename, 'r', encoding='utf-8') as file:\n",
        "        text_content = file.read()\n",
        "    print(f\"Read the contents of {txt_filename}\")\n",
        "\n",
        "    return text_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb-mXMSiTmA_",
        "outputId": "367f9147-4946-4c3b-edb5-8c469b20c10d"
      },
      "outputs": [],
      "source": [
        "raw_text = download_extract_read('https://download.pytorch.org/tutorial/data.zip', 'data.zip', 'data/eng-fra.txt')\n",
        "print(raw_text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlGqliYQTmBA"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6xmept-TmBB"
      },
      "source": [
        "Beforehand, we need to setup these parameters below. Since there are a lot of data and there are limited time and resource, we will filter the data to that has length of maximum 10. Other than that is the ID of pre-allocated tokens that we will use to build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2kG6MncHTmBB"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "MAX_LEN = 10\n",
        "PAD_TOKEN = 0\n",
        "BOS_TOKEN = 1\n",
        "EOS_TOKEN = 2\n",
        "UNK_TOKEN = 3\n",
        "\n",
        "\n",
        "set_random_seeds(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-QmGpWhTmBC"
      },
      "source": [
        "### Preprocess Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JXjZ-bHTmBD"
      },
      "source": [
        "For preprocessing, we mostly follow almost the same procedure as the pytorch Seq2Seq tutorial. We normalize the text and filter out some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8QSu92oXTmBD"
      },
      "outputs": [],
      "source": [
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def contains_only_valid_characters(text):\n",
        "    # Regular expression pattern to match allowed characters\n",
        "    pattern = r'^[a-zA-Z0-9.,!? \\t]*$'\n",
        "\n",
        "    # Check if the text matches the pattern\n",
        "    return bool(re.match(pattern, text))\n",
        "\n",
        "def split_source_target(raw_text, max_len):\n",
        "    sources, targets = [], []\n",
        "\n",
        "    # Split raw text into lines\n",
        "    lines = [line for line in raw_text.split('\\n')]\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) == 2:\n",
        "          src = normalizeString(parts[0])\n",
        "          tgt = normalizeString(parts[1])\n",
        "          src_tokens = src.split(' ')\n",
        "          tgt_tokens = tgt.split(' ')\n",
        "\n",
        "          # Filter out data longer than max_len\n",
        "          if (len(src_tokens) <= max_len) and (len(tgt_tokens) <= max_len) and src.startswith(eng_prefixes):\n",
        "              sources.append(src_tokens)\n",
        "              targets.append(tgt_tokens)\n",
        "\n",
        "    return sources, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIzu5v0UTmBE"
      },
      "source": [
        "### Build Language Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH0nuoAiTmBF"
      },
      "source": [
        "The `Lang` class is designed to handle language-specific tasks, building a vocabulary from a set of input data tokens. It manages the mapping between words (or tokens) and their corresponding integer indices, which is necessary for converting sequences of words into sequences of indices that can be used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zNTgAPiiTmBF"
      },
      "outputs": [],
      "source": [
        "class Lang():\n",
        "    def __init__(self, name, data_tokens):\n",
        "        self.name = name\n",
        "        self.index2word = {\n",
        "            0: '<PAD>',\n",
        "            1: '<BOS>',\n",
        "            2: '<EOS>',\n",
        "            3: '<UNK>'\n",
        "        }\n",
        "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "        self.word2count = {v: 0 for _, v in self.index2word.items()}\n",
        "        self.vocab_size = 4\n",
        "        self.add_data(data_tokens)\n",
        "\n",
        "    def add_data(self, data_tokens):\n",
        "        for tokens in data_tokens:\n",
        "            self.add_tokens(tokens)\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        for token in tokens:\n",
        "            if token not in self.word2index:\n",
        "                self.word2count[token] = 1\n",
        "                self.word2index[token] = self.vocab_size\n",
        "                self.index2word[self.vocab_size] = token\n",
        "                self.vocab_size += 1\n",
        "            else:\n",
        "                self.word2count[token] += 1\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.word2index.get(tokens, self.word2index['<UNK>'])\n",
        "        else:\n",
        "            return [self.__getitem__(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGE7H3bzTmBG"
      },
      "source": [
        "### Build Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwOc5bHkTmBG"
      },
      "source": [
        "This code below to convert sequences of text into padded tensors inside a DataLoader that can be fed into a neural network. You should understand what it does in details to the dataset and DataLoader since it is one of the important things to do this homework correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B3SqM2y0TmBH"
      },
      "outputs": [],
      "source": [
        "def pad_tensor(line, padding_token):\n",
        "    return line + [padding_token] * (MAX_LEN + 2 - len(line))\n",
        "\n",
        "def build_tensor(text, lang, is_source):\n",
        "    lines = [lang[line] for line in text]\n",
        "    if not is_source:\n",
        "        lines = [[lang['<BOS>']] + line + [lang['<EOS>']] for line in lines]\n",
        "    array = torch.tensor([pad_tensor(line, lang['<PAD>']) for line in lines])\n",
        "    valid_len = (array != lang['<PAD>']).sum(1)\n",
        "    return array, valid_len\n",
        "\n",
        "def create_data_loader(indices, batch_size, src_array, src_valid_len, tgt_array, tgt_valid_len, shuffle=True):\n",
        "        input_data = src_array[indices]\n",
        "        input_valid_len = src_valid_len[indices]\n",
        "        output_data = tgt_array[indices]\n",
        "        output_valid_len = tgt_valid_len[indices]\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(input_data, input_valid_len, output_data, output_valid_len)\n",
        "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "def load_data_nmt(source, target, src_vocab, tgt_vocab, batch_size=2):\n",
        "    rng = np.random.RandomState(SEED)\n",
        "\n",
        "    # Generate a random permutation of indices\n",
        "    indices = rng.permutation(len(source))\n",
        "\n",
        "    # Calculate split sizes\n",
        "    train_size = int(0.8 * len(source))\n",
        "    val_size = int(0.1 * len(source))\n",
        "\n",
        "    # Split indices based on the calculated sizes\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    src_array, src_valid_len = build_tensor(source, src_vocab, True)\n",
        "    tgt_array, tgt_valid_len = build_tensor(target, tgt_vocab, False)\n",
        "\n",
        "    train_iter = create_data_loader(train_indices, batch_size, src_array, src_valid_len, tgt_array, tgt_valid_len, shuffle=False)\n",
        "    val_iter = create_data_loader(val_indices, batch_size, src_array, src_valid_len, tgt_array, tgt_valid_len, shuffle=False)\n",
        "    test_iter = create_data_loader(test_indices, batch_size, src_array, src_valid_len, tgt_array, tgt_valid_len, shuffle=False)\n",
        "\n",
        "    return train_iter, val_iter, test_iter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6sxFJqSTmBI"
      },
      "source": [
        "Let's test the function to build the DataLoader and check how many data we got after filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INaKRBPSTmBI",
        "outputId": "a66c3aea-8537-4977-f676-15f35d943df1"
      },
      "outputs": [],
      "source": [
        "set_random_seeds(42)\n",
        "src, tgt = split_source_target(raw_text, MAX_LEN)\n",
        "\n",
        "print(len(src))\n",
        "\n",
        "eng_vocab = Lang('eng', src)\n",
        "fra_vocab = Lang('fra', tgt)\n",
        "\n",
        "train_iter, val_iter, test_iter = load_data_nmt(src, tgt, eng_vocab, fra_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ5NeT6ETmBJ"
      },
      "source": [
        "## Warm Up: Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt7Vs58nTmBK"
      },
      "source": [
        "To warm things up, let's implement a simple linear layer (fully connected layer) from scratch. You will write code that performs the forward pass for a linear transformation, applying the equation\n",
        "$$\n",
        "\\begin{align*}\n",
        "y = xW + b\n",
        "\\end{align*}\n",
        "$$\n",
        "Where:\n",
        "- $x$ is the input\n",
        "- $W$ is the weight matrix\n",
        "- $b$ is the bias\n",
        "\n",
        "Rules:\n",
        "- You are prohibited to use pytorch built-in linear layer `nn.Linear`\n",
        "- For $W_x$, initialize the weights **randomly** and **multiply it by 0.1 scale**\n",
        "- For $b_x$, must be **initialized to 0**\n",
        "\n",
        "Instructions:\n",
        "- Complete the `Linear` init() constructor and forward() forward pass function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PT4qyGSfTmBK"
      },
      "outputs": [],
      "source": [
        "class Linear(nn.Module):\n",
        "        def __init__(self, in_features, out_features, bias=True):\n",
        "                \"\"\"\n",
        "                Constructor for the Linear layer.\n",
        "\n",
        "                Args:\n",
        "                        in_features (int): The number of input features (i.e., the size of the input dimension).\n",
        "                        out_features (int): The number of output features (i.e., the size of the output dimension).\n",
        "                        bias (bool): If True, includes a bias term. Defaults to True.\n",
        "\n",
        "                Description:\n",
        "                        Initializes a linear layer that applies a linear transformation to the input. The weight\n",
        "                        matrix is initialized with random values scaled by 0.1, and the bias (if included) is initialized\n",
        "                        to zeros.\n",
        "                \"\"\"\n",
        "                super(Linear, self).__init__()\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the Linear constructor\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
        "                if bias:\n",
        "                    self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "                else:\n",
        "                    self.register_parameter('bias', None)\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "        def forward(self, x):\n",
        "                \"\"\"\n",
        "                Forward pass for the Linear layer.\n",
        "\n",
        "                Args:\n",
        "                        x (Tensor): Input tensor of shape (batch_size, in_features), where 'batch_size' is the number of examples,\n",
        "                                                and 'in_features' is the number of input features (matches in_features from the constructor).\n",
        "\n",
        "                Returns:\n",
        "                        Tensor: The output tensor of shape (batch_size, out_features), which is the result of applying\n",
        "                                        the linear transformation (weight matrix and optional bias) to the input.\n",
        "\n",
        "                Description:\n",
        "                        Performs the forward pass according to the equation.\n",
        "                \"\"\"\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the Linear forward pass\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                if self.bias is not None:\n",
        "                    output = x @ self.weight.T + self.bias\n",
        "                else:\n",
        "                    output = x @ self.weight.T\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "                return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLFc0HZTmBL"
      },
      "source": [
        "## RNN From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JkDJZeATmBM"
      },
      "source": [
        "In this section, you will implement various types of Recurrent Neural Networks (RNNs) from scratch, including Vanilla RNN, LSTM, GRU, and a Seq2Seq network with both Encoder and Decoder. This will give you a deeper understanding of how these architectures work internally and how different RNN variants handle sequences.\n",
        "\n",
        "Rules:\n",
        "- Do not use PyTorch's built-in classes like `nn.RNN`, `nn.LSTM`, `nn.LSTMCell`, etc.\n",
        "- Do not use `nn.Linear`; instead, use the `Linear` layer you previously implemented.\n",
        "- You are allowed to use other PyTorch utilities like `nn.Embedding` where appropriate.\n",
        "- Initialize all weights randomly and scale them by 0.1.\n",
        "- Set all biases to 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihk4I5EhTmBM"
      },
      "source": [
        "### Vanilla RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEupvToaTmBN"
      },
      "source": [
        "The Vanilla RNN Cell processes data for one time step and return the hidden state. You'll implement the forward pass using the following equations:\n",
        "$$\n",
        "\\begin{align*}\n",
        "h_t &= \\text{tanh}( x_tW_{hx} + h_{t-1}W_{hh} + b_h ) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $x_t$ is input at time step $t$\n",
        "- $h_{t-1}$ is hidden state at previous time step $t-1$\n",
        "- $W_{hx}, W_{hh}$ are the weight matrices\n",
        "- $b_h$ is the bias\n",
        "\n",
        "Instructions:\n",
        "- Complete the `VanillaRNNCell` init() constructor and forward() forward pass for one time step function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tXyJPX1XTmBO"
      },
      "outputs": [],
      "source": [
        "class VanillaRNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"\n",
        "        Constructor for the VanillaRNNCell class.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The size (number of features) of the input data at each time step.\n",
        "            hidden_size (int): The size (number of features) of the hidden state.\n",
        "\n",
        "        Description:\n",
        "            Initializes a vanilla RNN cell, which will compute the hidden state for one time step.\n",
        "            The RNN cell uses two sets of weights: one for the input-to-hidden transformation (W_xh),\n",
        "            and one for the hidden-to-hidden transformation (W_hh).\n",
        "        \"\"\"\n",
        "        super(VanillaRNNCell, self).__init__()\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the VanillaRNNCell constructor\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        # self.W_hx = nn.Parameter(torch.randn(hidden_size, input_size) * 0.1) # weight for input\n",
        "        # self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1) # weight for hidden state\n",
        "        # self.b_h = nn.Parameter(torch.zeros(hidden_size)) # bias for hidden state\n",
        "\n",
        "        self.W_hx = Linear(input_size, hidden_size)\n",
        "        self.W_hh = Linear(hidden_size, hidden_size)\n",
        "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "    def forward(self, x_t, h_t_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for one time step.\n",
        "        Args:\n",
        "            x_t (Tensor): Input at time t (batch_size, input_size).\n",
        "            h_t_prev (Tensor): Hidden state at time t-1 (batch_size, hidden_size).\n",
        "        Returns:\n",
        "            h_t (Tensor): Hidden state at time t (batch_size, hidden_size).\n",
        "        \"\"\"\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the VanillaRNNCell forward pass\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        h_t = torch.tanh(self.W_hx(x_t) + self.W_hh(h_t_prev) + self.b_h)\n",
        "\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "        return h_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMlWyO6YTmBP"
      },
      "source": [
        "Now, by using the implemented `VanillaRNNCell`, implement the `VanillaRNN` which is the RNN that runs through the whole sequence.\n",
        "\n",
        "Important things:\n",
        "- If `h_0` is `None` then you should initialize the hidden state with 0 tensor.\n",
        "- When running the forward pass, you should consider the valid lengths of the sequence, which you can found on the arg `valid_len`.\n",
        "- Any kind of update to the hidden and cell state **beyond the valid lengths** are prohibited, to make sure that the last hidden and cell state are valid.\n",
        "- Therefore, $h_t$ must not be updated if $t \\geq \\text{valid\\_len}$\n",
        "\n",
        "<div>\n",
        "<img src=\"https://ethen8181.github.io/machine-learning/deep_learning/rnn/img/rnn_unrolled.png\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://ethen8181.github.io/machine-learning/deep_learning/rnn/img/rnn_unrolled.png\n",
        "\n",
        "Instructions:\n",
        "- Utilize the `VanillaRNNCell` to implement the `VanillaRNN` init() constructor and forward() forward pass through the whole sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1_7LGDQ1TmBQ"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size):\n",
        "                \"\"\"\n",
        "                Constructor for the VanillaRNN class.\n",
        "\n",
        "                Args:\n",
        "                        input_size (int): The size (number of features) of the input data at each time step.\n",
        "                        hidden_size (int): The size (number of features) of the hidden state.\n",
        "\n",
        "                Description:\n",
        "                        Initializes the VanillaRNN model, which processes sequential input data across multiple time steps.\n",
        "                        This class uses the previously defined VanillaRNNCell to process one time step at a time.\n",
        "                        The RNN will maintain a hidden state of size `hidden_size` throughout the sequence.\n",
        "                \"\"\"\n",
        "                super(VanillaRNN, self).__init__()\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the VanillaRNN constructor\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                self.hidden_size = hidden_size\n",
        "                self.rnn_cell = VanillaRNNCell(input_size, hidden_size)\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "        def forward(self, x, h_0=None, valid_len=None, **kwargs):\n",
        "                \"\"\"\n",
        "                Forward pass for the entire sequence.\n",
        "                Args:\n",
        "                        x (Tensor): Input sequence of shape (batch_size, seq_length, input_size).\n",
        "                        h_0 (Tensor, optional): Initial hidden state of shape (batch_size, hidden_size).\n",
        "                        valid_len (Tensor): Tensor of shape (batch_size,) containing the lengths of sequences before padding.\n",
        "                Returns:\n",
        "                        h_seq (Tensor): Hidden states at each time step of shape (batch_size, seq_length, hidden_size).\n",
        "                        h_t (Tensor): The last hidden state for last time step of shape (batch_size, hidden_size)\n",
        "                \"\"\"\n",
        "                # print(\"나야\")\n",
        "                # print(x.size())\n",
        "\n",
        "                batch_size, seq_length, _ = x.size()\n",
        "\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the VanillaRNN forward pass. If h_0 is None, then the initial\n",
        "                # hidden state should be initialized to 0 tensor. Also, implement the masking based\n",
        "                # on the given valid_len if it is not None.\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                if h_0 is None:\n",
        "                    h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "                else:\n",
        "                    h_t = h_0\n",
        "\n",
        "                h_seq = torch.zeros(batch_size, seq_length, self.hidden_size).to(x.device)\n",
        "\n",
        "                for t in range(seq_length):\n",
        "                    if valid_len is not None:\n",
        "                        mask = (valid_len > t).float().unsqueeze(1)\n",
        "\n",
        "                        h_t = self.rnn_cell(x[:, t, :], h_t) * mask + h_t.detach() * (1 - mask)\n",
        "                        # x[:, t, :] : 3D 텐서에서 특정한 시간 차원 t에 해당되는 값을 선택  (seq_length = t)\n",
        "                    else:\n",
        "                        h_t = self.rnn_cell(x[:, t, :], h_t)\n",
        "\n",
        "                    h_seq[:, t, :] = h_t\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "                return h_seq, h_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEBenVhpTmBS"
      },
      "source": [
        "### LSTM\n",
        "\n",
        "You'll implement a Long Short-Term Memory (LSTM) cell from scratch. LSTMs are powerful recurrent neural networks that can capture long-term dependencies in sequential data.\n",
        "LSTM Equations\n",
        "The LSTM cell is defined by the following equations:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{Input Gate} &&& I_t &= \\sigma( x_tW_{xi} + h_{t-1}W_{hi} + b_i ) \\\\\n",
        "&\\text{Forget Gate} &&& F_t &= \\sigma( x_tW_{xf} + h_{t-1}W_{hf} + b_f ) \\\\\n",
        "&\\text{Output Gate} &&& O_t &= \\sigma( x_tW_{xo} + h_{t-1}W_{ho} + b_o ) \\\\\n",
        "&\\text{Cell State Candidate} &&& \\tilde{C}_t &= \\text{tanh}( x_tW_{xc} + h_{t-1}W_{hc} + b_c ) \\\\\n",
        "&\\text{Updated Cell State} &&& C_t &= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\\n",
        "&\\text{Hidden State} &&& h_t &= O_t \\odot \\text{tanh}(C_t) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\sigma$ is the sigmoid function\n",
        "- $\\odot$ denotes element-wise multiplication\n",
        "\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/450px-LSTM_Cell.svg.png\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/450px-LSTM_Cell.svg.png\n",
        "\n",
        "Instructions:\n",
        "- Complete the `LSTMCell` init() constructor and forward() forward pass for one time step function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "81sriwkUTmBT"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"\n",
        "        Constructor for the LSTMCell class.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The number of features in the input data at each time step.\n",
        "            hidden_size (int): The number of features in the hidden state.\n",
        "\n",
        "        Description:\n",
        "            Initializes the LSTM cell, which processes one time step of the input and updates both\n",
        "            the hidden state (h_t) and cell state (c_t). LSTM cells have three gates: forget, input,\n",
        "            and output gates, which control the flow of information through the cell.\n",
        "        \"\"\"\n",
        "        super(LSTMCell, self).__init__()\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the LSTMCell constructor\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # weight for input gate\n",
        "        \n",
        "        # self.W_xi = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1) # 정규분포를 따르는 랜덤 값을 생성하여 텐서를 반환.\n",
        "        # self.W_hi = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        # self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_xi = Linear(input_size, hidden_size)\n",
        "        self.W_hi = Linear(hidden_size, hidden_size)\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # weight for forget gate\n",
        "        # self.W_xf = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        # self.W_hf = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        # self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_xf = Linear(input_size, hidden_size)\n",
        "        self.W_hf = Linear(hidden_size, hidden_size)\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # weights for output gate\n",
        "        # self.W_xo = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        # self.W_ho = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        # self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_xo = Linear(input_size, hidden_size)\n",
        "        self.W_ho = Linear(hidden_size, hidden_size)\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        #weights for cell state candidate\n",
        "        # self.W_xc = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        # self.W_hc = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        # self.b_c = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_xc = Linear(input_size, hidden_size)\n",
        "        self.W_hc = Linear(hidden_size, hidden_size)\n",
        "        self.b_c = nn.Parameter(torch.zeros(hidden_size))\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "    def forward(self, x_t, h_t_prev, c_t_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for one time step of LSTM.\n",
        "        Args:\n",
        "            x_t (Tensor): Input at time t (batch_size, input_size).\n",
        "            h_t_prev (Tensor): Hidden state at time t-1 (batch_size, hidden_size).\n",
        "            c_t_prev (Tensor): Cell state at time t-1 (batch_size, hidden_size).\n",
        "        Returns:\n",
        "            h_t (Tensor): Hidden state at time t (batch_size, hidden_size).\n",
        "            c_t (Tensor): Cell state at time t (batch_size, hidden_size).\n",
        "        \"\"\"\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the LSTMCell forward pass. Just implement it based the equation on\n",
        "        # the equation and you should be fine.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        # Input gate\n",
        "        # I_t = torch.sigmoid(x_t @ self.W_xi + h_t_prev @ self.W_hi + self.b_i)\n",
        "        I_t = torch.sigmoid(self.W_xi(x_t) + self.W_hi(h_t_prev) + self.b_i)\n",
        "\n",
        "        # Forget gate\n",
        "        # F_t = torch.sigmoid(x_t @ self.W_xf + h_t_prev @ self.W_hf + self.b_f)\n",
        "        F_t = torch.sigmoid(self.W_xf(x_t) + self.W_hf(h_t_prev) + self.b_f)\n",
        "\n",
        "        # Output gate\n",
        "        # O_t = torch.sigmoid(x_t @ self.W_xo + h_t_prev @ self.W_ho + self.b_o)\n",
        "        O_t = torch.sigmoid(self.W_xo(x_t) + self.W_ho(h_t_prev) + self.b_o)\n",
        "\n",
        "        # Cell state cndidate\n",
        "        # C_state_candidate = torch.tanh(x_t @ self.W_xc + h_t_prev @ self.W_hc + self.b_c)\n",
        "        C_state_candidate = torch.tanh(self.W_xc(x_t) + self.W_hc(h_t_prev) + self.b_c)\n",
        "\n",
        "        # Updated cell stae\n",
        "        c_t = F_t * c_t_prev + I_t * C_state_candidate\n",
        "\n",
        "        # Updated hidden state\n",
        "        h_t = O_t * torch.tanh(c_t)\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "        return h_t, c_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cps1a21TmBm"
      },
      "source": [
        "Now, by using the implemented `LSTMCell`, implement the `LSTM` which is the LSTM that runs through the whole sequence.\n",
        "\n",
        "Important things:\n",
        "- If `h_0` or `c_0` is `None` then you should initialize them with 0 tensor.\n",
        "- When running the forward pass, you should consider the valid lengths of the sequence, which you can found on the arg `valid_len`.\n",
        "- Any kind of update to the hidden and cell state **beyond the valid lengths** are prohibited, to make sure that the last hidden and cell state are valid.\n",
        "- Therefore, $C_t$ and $h_t$ must not be updated if $t \\geq \\text{valid\\_len}$\n",
        "\n",
        "Instructions:\n",
        "- Complete the `LSTM` init() constructor and forward() forward pass through the whole sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m0nK_ZLyTmBo"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"\n",
        "        Constructor for the LSTM class.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The number of features in the input data at each time step.\n",
        "            hidden_size (int): The number of features in the hidden state.\n",
        "\n",
        "        Description:\n",
        "            Initializes the LSTM model, which processes sequential data across multiple time steps.\n",
        "            This class uses the previously defined LSTMCell to compute the hidden and cell states at each\n",
        "            time step. It maintains a hidden state and a cell state of size `hidden_size` throughout the\n",
        "            sequence processing.\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the LSTM constructor\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_cell = LSTMCell(input_size, hidden_size)\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "    def forward(self, x, h_0=None, c_0=None, valid_len=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire sequence.\n",
        "        Args:\n",
        "            x (Tensor): Input sequence of shape (batch_size, seq_length, input_size).\n",
        "            h_0 (Tensor, optional): Initial hidden state of shape (batch_size, hidden_size).\n",
        "            c_0 (Tensor, optional): Initial cell state of shape (batch_size, hidden_size).\n",
        "            valid_len (Tensor, optional): Tensor containing the lengths of sequences before padding of shape (batch_size,).\n",
        "        Returns:\n",
        "            h_seq (Tensor): Hidden states at each time step with shape (batch_size, seq_length, hidden_size).\n",
        "            (h_t, c_t) (Tuple): Final hidden and cell states with both shape (batch_size, 1, hidden_size).\n",
        "        \"\"\"\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the LSTM forward pass. Remember to initialize hidden and cell\n",
        "        # state is not prodided and apply the mask to prevent prohibited updates.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        if h_0 is None:\n",
        "          h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "        else:\n",
        "          h_t = h_0\n",
        "\n",
        "        if c_0 is None:\n",
        "          c_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "        else:\n",
        "          c_t = c_0\n",
        "\n",
        "        h_seq = torch.zeros(batch_size, seq_length, self.hidden_size).to(x.device)\n",
        "\n",
        "        for t in range(seq_length):\n",
        "            if valid_len is not None:\n",
        "                mask = (valid_len > t).float().unsqueeze(1)\n",
        "            else:\n",
        "                mask = torch.ones(batch_size, 1, device = x.device)\n",
        "\n",
        "            h_t_new, c_t_new = self.lstm_cell(x[:,t, :], h_t, c_t)\n",
        "            h_t = h_t_new * mask + h_t.detach() * (1 - mask)\n",
        "            c_t = c_t_new * mask + c_t.detach() * (1 - mask)\n",
        "\n",
        "\n",
        "            h_seq[:, t, :] = h_t\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "        return h_seq, (h_t, c_t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tjDTPOdTmBp"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuZoFI12TmBq"
      },
      "source": [
        "You'll implement a Gated Recurrent Unit (GRU) cell from scratch. GRU is a variation of LSTM with fewer parameters and comparable performance, making it an efficient choice for many sequence modeling tasks. The GRU cell is defined by the following equations:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{Reset Gate} &&& R_t &= \\sigma( x_tW_{xr} + h_{t-1}W_{hr} + b_r ) \\\\\n",
        "&\\text{Update Gate} &&& Z_t &= \\sigma( x_tW_{xz} + h_{t-1}W_{hz} + b_z ) \\\\\n",
        "&\\text{Hidden State Candidate} &&& \\tilde{h}_t &= \\text{tanh}( x_tW_{xh} + (R_t \\odot h_{t-1})W_{hh} + b_h ) \\\\\n",
        "&\\text{Hidden States} &&& h_t &= (1 - Z_t) \\odot h_{t-1} + Z_t \\odot \\tilde{h}_t \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\sigma$ is the sigmoid function\n",
        "- $\\odot$ denotes element-wise multiplication\n",
        "\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Gated_Recurrent_Unit%2C_base_type.svg/1920px-Gated_Recurrent_Unit%2C_base_type.svg.png\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Gated_Recurrent_Unit%2C_base_type.svg/1920px-Gated_Recurrent_Unit%2C_base_type.svg.png\n",
        "\n",
        "Instructions:\n",
        "- Complete the `GRUCell` init() constructor and forward() forward pass for one time step function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vq8kp_7ATmBq"
      },
      "outputs": [],
      "source": [
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"\n",
        "        Constructor for the GRUCell class.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The number of features in the input data at each time step.\n",
        "            hidden_size (int): The number of features in the hidden state.\n",
        "\n",
        "        Description:\n",
        "            Initializes a Gated Recurrent Unit (GRU) cell, which processes one time step of input and updates\n",
        "            the hidden state. A GRU cell uses two gates: an update gate and a reset gate, which control the\n",
        "            flow of information, and a candidate hidden state that proposes a new hidden state value.\n",
        "        \"\"\"\n",
        "        super(GRUCell, self).__init__()\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the GRUCell constructor\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        # reset gate 가중치\n",
        "        self.W_xr = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.W_hr = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_r = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # update gate 가중치\n",
        "        self.W_xz = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.W_hz = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_z = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # hidden state candidate 가중치\n",
        "        self.W_xh = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "    def forward(self, x_t, h_t_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for one time step of GRU.\n",
        "        Args:\n",
        "            x_t (Tensor): Input at time t (batch_size, input_size).\n",
        "            h_t_prev (Tensor): Hidden state at time t-1 (batch_size, hidden_size).\n",
        "        Returns:\n",
        "            h_t (Tensor): Hidden state at time t (batch_size, hidden_size).\n",
        "        \"\"\"\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the GRUCell forward pass. Follow the provided equation.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        # Reset gate\n",
        "        R_t = torch.sigmoid(x_t @ self.W_xr + h_t_prev @ self.W_hr + self.b_r)\n",
        "        Z_t = torch.sigmoid(x_t @ self.W_xz + h_t_prev @ self.W_hz + self.b_z)\n",
        "\n",
        "        hidden_state_candidate = torch.tanh(x_t @ self.W_xh + (R_t * h_t_prev) @ self.W_hh + self.b_h)\n",
        "\n",
        "        h_t = (1 - Z_t) * h_t_prev + Z_t * hidden_state_candidate\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "        return h_t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOzACuAjTmBr"
      },
      "source": [
        "Now, by using the implemented `GRUCell`, implement the `GRU` which is the GRU that runs through the whole sequence.\n",
        "\n",
        "Important things:\n",
        "- If `h_0` is `None` then you should initialize the hidden state with 0 tensor.\n",
        "- When running the forward pass, you should consider the valid lengths of the sequence, which you can found on the arg `valid_len`.\n",
        "- Any kind of update to the hidden and cell state **beyond the valid lengths** are prohibited, to make sure that the last hidden and cell state are valid.\n",
        "- Therefore, $h_t$ must not be updated if $t \\geq \\text{valid\\_len}$\n",
        "\n",
        "Instructions:\n",
        "- Complete the `GRU` init() constructor and forward() forward pass through the whole sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uEbFFIN4TmBs"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size):\n",
        "                super(GRU, self).__init__()\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the GRU constructor\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                self.input_size = input_size\n",
        "                self.hidden_size = hidden_size\n",
        "                self.gru_cell = GRUCell(input_size, hidden_size)\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "        def forward(self, x, h_0=None, valid_len=None, **kwargs):\n",
        "                \"\"\"\n",
        "                Forward pass for the entire sequence.\n",
        "                Args:\n",
        "                        x (Tensor): Input sequence of shape (batch_size, seq_length, input_size).\n",
        "                        h_0 (Tensor, optional): Initial hidden state of shape (batch_size, hidden_size).\n",
        "                        valid_len (Tensor, optional): Tensor containing the lengths of sequences before padding of shape (batch_size,).\n",
        "                Returns:\n",
        "                        h_seq (Tensor): Hidden states at each time step of shape (batch_size, seq_length, hidden_size).\n",
        "                        h_t: Final hidden state (batch_size, hidden_size).\n",
        "                \"\"\"\n",
        "                batch_size, seq_length, _ = x.size()\n",
        "\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the GRU forward pass. Do not forget to initialize hidden state if\n",
        "                # not any and prevent unwanted state updates.\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                if h_0 is None:\n",
        "                    h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "                else:\n",
        "                    h_t = h_0\n",
        "\n",
        "                h_seq = torch.zeros(batch_size, seq_length, self.hidden_size).to(x.device)\n",
        "\n",
        "                for t in range(seq_length):\n",
        "                    if valid_len is not None:\n",
        "                        mask = (valid_len > t).float().unsqueeze(1)\n",
        "                        h_t = self.gru_cell(x[:, t, :], h_t) * mask + h_t.detach() * (1 - mask)\n",
        "                    else:\n",
        "                        h_t = self.gru_cell(x[:, t, :], h_t)\n",
        "\n",
        "                    h_seq[:, t, :] = h_t\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "                return h_seq, h_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0-7AF_qTmBt"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgJrJtcsTmBt"
      },
      "source": [
        "In this task, you will implement an RNNEncoder that processes sequences of word indices and returns the final hidden (and cell) states. Your implementation of this RNNEncoder must be able to use any RNN variant you've previously implemented, such as Vanilla RNN, LSTM, or GRU.\n",
        "\n",
        "Key components of the encoder:\n",
        "- **Embedding layer**: Transforms word indices into dense vectors.\n",
        "- **RNN layer**: Applies your custom RNN implementation (without using PyTorch's built-in RNN classes).\n",
        "\n",
        "The forward pass takes in the input sequences and sequence lengths, then outputs the final hidden state (and cell state if using LSTM). Follow the given code structure to integrate your RNN variants into this encoder.\n",
        "\n",
        "Instructions:\n",
        "- Complete the `RNNEncoder` init() constructor and forward() forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u0e5abWvTmBu"
      },
      "outputs": [],
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "        def __init__(self, vocab_size, embed_dim, hidden_dim, rnn_net):\n",
        "                super(RNNEncoder, self).__init__()\n",
        "                \"\"\"\n",
        "                Args:\n",
        "                        vocab_size (int): Number of unique words in the source vocabulary.\n",
        "                        embed_dim (int): Dimension of the word embeddings.\n",
        "                        hidden_dim (int): Dimension of the hidden state in the RNN.\n",
        "                        rnn_net: Any of your RNN variants that already implemented.\n",
        "                \"\"\"\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the RNNEncoder constructor. You can use `nn.Embedding` for the\n",
        "                # embedding layer.\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "                self.rnn_net = rnn_net\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "        def forward(self, input_seq, valid_len):\n",
        "                \"\"\"\n",
        "                Args:\n",
        "                        input_seq (Tensor): Tensor of shape (batch_size, sequence_length) containing word indices.\n",
        "                        valid_len (Tensor): Tensor of shape (batch_size,) containing the lengths of sequences before padding.\n",
        "\n",
        "                Returns:\n",
        "                        final_hidden (Tensor): Tensor of shape (batch_size, hidden_dim) representing the final hidden states.\n",
        "                        final_cell (Tensor or None): Either\n",
        "                            1. Tensor of shape (batch_size, hidden_dim) representing the final cell states if the rnn_net is LSTM.\n",
        "                            2. None if the rnn_net is not LSTM\n",
        "                \"\"\"\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the RNNEncoder forward pass. You must embed the input sequence to\n",
        "                # before put it into the RNN layer. Remember, output of RNN layer can be different\n",
        "                # depending on the type of RNN. Your code must be able to handle all of the\n",
        "                # implemented RNN variants.\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                embedded = self.embedding(input_seq)\n",
        "\n",
        "                if isinstance(self.rnn_net, LSTM): # isinstance(object, classinfo) -> classinfo 안에 object가 있는가?\n",
        "                    h_seq, (final_hidden, final_cell) = self.rnn_net(embedded, valid_len=valid_len)\n",
        "                else:\n",
        "                    h_seq, final_hidden = self.rnn_net(embedded, valid_len=valid_len)\n",
        "                    final_cell = None\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "                return final_hidden, final_cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7zb-I_YTmBv"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgyIE_JDTmBw"
      },
      "source": [
        "In this task, you will implement an RNNDecoder that generates sequences of word indices based on an initial hidden (and cell) state, using any of the RNN variants you have previously implemented (Vanilla RNN, LSTM, GRU). The decoder can operate in two modes:\n",
        "- **Teacher Forcing**: When a target sequence is provided, the decoder uses the target tokens as inputs for each time step.\n",
        "- **Inference**: When no target sequence is given, the decoder generates tokens one by one, feeding the predicted token from the previous step as the input for the next.\n",
        "\n",
        "The decoder consists of:\n",
        "- An **embedding layer** to convert word indices into dense vectors.\n",
        "- Your **custom RNN layer** (without using PyTorch's built-in RNNs).\n",
        "- An **output layer** to map hidden states to vocabulary scores.\n",
        "\n",
        "Follow the structure provided to implement this functionality, ensuring you apply the RNN and sequence generation logic correctly.\n",
        "\n",
        "Instructions:\n",
        "- Complete the `RNNDecoder` init() constructor and forward() forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F7fBbaMATmBw"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "        def __init__(self, vocab_size, embed_dim, hidden_dim, rnn_net):\n",
        "                super(RNNDecoder, self).__init__()\n",
        "                \"\"\"\n",
        "                Args:\n",
        "                        vocab_size (int): Number of unique words in the target vocabulary.\n",
        "                        embed_dim (int): Dimension of the word embeddings.\n",
        "                        hidden_dim (int): Dimension of the hidden state in the RNN.\n",
        "                \"\"\"\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the RNNDecoder constructor. You can use `nn.Embedding` for the\n",
        "                # embedding layer. Do not use `nn.Linear`, use your implemented Linear class instead\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "                self.rnn_net = rnn_net\n",
        "                self.output_layer = Linear(hidden_dim, vocab_size)\n",
        "                self.vocab_size = vocab_size\n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "        def forward(self, initial_hidden, initial_cell=None, target_seq=None):\n",
        "                \"\"\"\n",
        "                Args:\n",
        "                        initial_hidden (Tensor): Tensor of shape (batch_size, hidden_dim) representing the initial hidden state.\n",
        "                        initial_cell (Tensor or None): Either\n",
        "                            1. Tensor of shape (batch_size, hidden_dim) representing the initial cell state if rnn_net is LSTM.\n",
        "                            2. None if rnn_net is not LSTM\n",
        "                        target_seq (Tensor): Tensor of shape (batch_size, target_sequence_length) containing word indices.\n",
        "\n",
        "                Returns:\n",
        "                        outputs (Tensor): Tensor of shape (batch_size, target_sequence_length, vocab_size) with raw scores.\n",
        "                        hidden_state (Tensor): Tensor containing the final hidden state.\n",
        "                \"\"\"\n",
        "                batch_size = initial_hidden.size(0)\n",
        "\n",
        "                if target_seq is not None:\n",
        "                        ####################################################################################\n",
        "                        # TODO: Implement the RNNDecoder forward pass when the target_seq is available. That\n",
        "                        # means you must implement the forward pass using teacher forcing, where you force\n",
        "                        # the input of every time step according to the target sequence provided, instead of\n",
        "                        # using the previously predicted token as input.\n",
        "                        ####################################################################################\n",
        "\n",
        "                        # Your implementation code\n",
        "                        target_length = target_seq.size(1)\n",
        "                        outputs = torch.zeros(batch_size, target_length, self.vocab_size, device = initial_hidden.device) # 할당한 텐서를 target_seq와 동일한 device에 배치\n",
        "                        hidden_state = initial_hidden\n",
        "                        cell_state = initial_cell\n",
        "\n",
        "                        for t in range(target_length):\n",
        "                            embedded_input = self.embedding(target_seq[:, t])\n",
        "                            if isinstance(self.rnn_net, LSTM): # rnn_net이 LSTM일 때\n",
        "                                output, (hidden_state, cell_state) = self.rnn_net(embedded_input.unsqueeze(1), hidden_state, cell_state)\n",
        "\n",
        "\n",
        "                            else:\n",
        "                                output, hidden_state = self.rnn_net(embedded_input.unsqueeze(1), hidden_state)\n",
        "\n",
        "                            outputs[:, t, :] = self.output_layer(output.squeeze(1)) \n",
        "                            # shift-righted target_seq. \n",
        "                            # teacher forcing tgt sequence가 <sos> <t1> <t2> <t3> <t4> <eos> 일 때,\n",
        "                            #  output은 <t1> <t2> <t3> <t4> <eos> <pad>가 나와야 함\n",
        "\n",
        "\n",
        "                        # bos_score = torch.zeros(self.vocab_size)\n",
        "                        # bos_score[1] = 1\n",
        "                        # outputs[:, 0, :] = bos_score.to(initial_hidden.device)\n",
        "\n",
        "\n",
        "                        ####################################################################################\n",
        "                        # END OF YOUR CODE\n",
        "                        ####################################################################################\n",
        "\n",
        "                        return outputs, hidden_state\n",
        "                else:\n",
        "                        ####################################################################################\n",
        "                        # TODO: Implement the RNNDecoder forward pass when the target_seq is not available.\n",
        "                        # In this case, the model runs in inference mode. Therefore you should use the\n",
        "                        # previously predicted token as the input token to predict the next token. Also, do\n",
        "                        # not forget that the first input is always a BOS_TOKEN, which you must use at the\n",
        "                        # first time step.\n",
        "                        ####################################################################################\n",
        "\n",
        "                        # Your implementation code\n",
        "                        bos_token = torch.tensor([BOS_TOKEN] * batch_size).to(initial_hidden.device) # size = (batch_size, )\n",
        "                        embedded_input = self.embedding(bos_token) # 초기 embedded_input\n",
        "                        # print(embedded_input.size()) : [64, 64] = (batchsize, embedding_dim)\n",
        "\n",
        "                        hidden_state = initial_hidden\n",
        "                        cell_state = initial_cell if isinstance(self.rnn_net, LSTM) else None # if LSTM, cell_state = initial_cell\n",
        "\n",
        "                        \n",
        "                        outputs = torch.zeros(batch_size, MAX_LEN + 2, self.vocab_size, device = initial_hidden.device) # size(batch_size, max_length, vocab_size)\n",
        "                        # outputs[:, 0, BOS_TOKEN] = 1\n",
        "\n",
        "                        for t in range(MAX_LEN):\n",
        "                            if isinstance(self.rnn_net, LSTM):\n",
        "                                output, (hidden_state, cell_state) = self.rnn_net(embedded_input.unsqueeze(1), hidden_state, cell_state)\n",
        "                            else:\n",
        "                                output, hidden_state = self.rnn_net(embedded_input.unsqueeze(1), hidden_state)\n",
        "                                # output.size() = (batch_size, seq_length, hidden_size). -> VanillaRNN에서 h_seq (time step t마다 hidden state 모아놓음)\n",
        "                                # hidden_state.size() = (batch_size, hidden_size) -> 가장 마지막 hidden state\n",
        "\n",
        "                            output_scores = self.output_layer(output.squeeze(1))\n",
        "                            # output_scores = self.output_layer(output.squeeze(1)) # 시그모이드 적용\n",
        "                            outputs[:, t, :] = output_scores\n",
        "                            # outputs size = (batch_size, target_sequence_length, vocab_size)\n",
        "\n",
        "                            predicted_token = torch.argmax(output_scores, dim=-1)\n",
        "                            #print(predicted_token) -> 얘도 다 1만 출력\n",
        "                            # print(predicted_token.size()) = [64]\n",
        "                            embedded_input = self.embedding(predicted_token)\n",
        "\n",
        "                        # bos_score = torch.zeros(self.vocab_size)\n",
        "                        # bos_score[BOS_TOKEN] = 1\n",
        "                        # outputs[:, 0, :] = bos_score.to(initial_hidden.device)\n",
        "\n",
        "                        # eos_score = torch.zeros(self.vocab_size)\n",
        "                        # eos_score[EOS_TOKEN] = 1\n",
        "                        # outputs[:, -1, :] = eos_score.to(initial_hidden.device)\n",
        "\n",
        "\n",
        "                        ####################################################################################\n",
        "                        # END OF YOUR CODE\n",
        "                        ####################################################################################\n",
        "                        \n",
        "                        \n",
        "                        # output은 <sos>를 포함하지 않음. tgt_seq가 1 token만큼 right-shifted 되어 있음.\n",
        "                        # target_seq : <sos> <t1> <t2> <t3> <t4> <eos> -> output : <t1> <t2> <t3> <t4> <eos> <pad>\n",
        "                        return outputs, hidden_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyrZjI0PTmBy"
      },
      "source": [
        "### RNNTranslator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUCxdkDOTmBz"
      },
      "source": [
        "Here, you will implement an RNNTranslator that combines the encoder-decoder architecture to perform sequence-to-sequence translation. The translator uses your custom RNNEncoder and RNNDecoder implementations (e.g., Vanilla RNN, LSTM, GRU) to translate a sequence from a source language to a target language.\n",
        "\n",
        "Key components of the RNNTranslator:\n",
        "- **Encoder**: Encodes the source sequence into a final hidden state (and cell state if using LSTM).\n",
        "- **Decoder**: Generates the target sequence based on the encoder's output and the target sequence during training.\n",
        "\n",
        "The model includes:\n",
        "- A `forward` method to handle the translation during training with teacher forcing.\n",
        "- A `predict` method to perform inference and generate translations step-by-step.\n",
        "\n",
        "Your task is to implement the translation process by linking the encoder and decoder together, ensuring the hidden states are passed correctly between them.\n",
        "\n",
        "Instructions:\n",
        "- Complete the `RNNTranslator` init(), forward(), and predict() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ADtB5IJGTmB0"
      },
      "outputs": [],
      "source": [
        "class RNNTranslator(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, enc_net, dec_net):\n",
        "    super(RNNTranslator, self).__init__()\n",
        "  ####################################################################################\n",
        "  # TODO: Implement the RNNTranslator constructor\n",
        "  ####################################################################################\n",
        "\n",
        "  # Your implementation code\n",
        "  # Encoder-Decoder 구조를 사용하여 seq2seq 번역작업을 수행하는 RNNTranslator 구현하기.\n",
        "    self.enc_emb = nn.Embedding(src_vocab_size, embedding_dim)\n",
        "    self.dec_emb = nn.Embedding(tgt_vocab_size, embedding_dim)\n",
        "\n",
        "    self.encoder = RNNEncoder(src_vocab_size, embedding_dim, hidden_size, enc_net)\n",
        "    self.decoder = RNNDecoder(tgt_vocab_size, embedding_dim, hidden_size, dec_net)\n",
        "\n",
        "  # self.enc_net = enc_net\n",
        "  # self.dec_net = dec_net\n",
        "\n",
        "\n",
        "  # self.output_layer = nn.Linear(hidden_size, tgt_vocab_size)\n",
        "\n",
        "  ####################################################################################\n",
        "  # END OF YOUR CODE\n",
        "  ####################################################################################\n",
        "\n",
        "  def forward(self, source_seq, source_lengths, target_seq, target_lengths):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      source_seq (Tensor): Source sequences (batch_size, source_sequence_length).\n",
        "      source_lengths (Tensor): Lengths of source sequences before padding (batch_size,).\n",
        "      target_seq (Tensor): Target sequences (batch_size, target_sequence_length).\n",
        "      target_lengths (Tensor): Lengths of target sequences before padding (batch_size,).\n",
        "\n",
        "    Returns:\n",
        "      logits (Tensor): Logits of shape (batch_size, tgt_seq_len, tgt_vocab_size), representing predicted token probabilities.\n",
        "    \"\"\"\n",
        "    batch_size = source_seq.size(0)\n",
        "\n",
        "    ####################################################################################\n",
        "    # TODO: Implement the RNNTranslator forward pass by running the forward pass of the\n",
        "    # Encoder continued by the Decoder.\n",
        "    ####################################################################################\n",
        "\n",
        "    # Your implementation code\n",
        "    final_hidden, final_cell = self.encoder(source_seq, source_lengths)\n",
        "\n",
        "    # 2. Decoder에 target sequence와 encoder로부터 받은 hidden state와 cell state를 넘겨줌\n",
        "    logits, _ = self.decoder(final_hidden, initial_cell = final_cell, target_seq = target_seq)\n",
        "\n",
        "\n",
        "    ####################################################################################\n",
        "    # END OF YOUR CODE\n",
        "    ####################################################################################\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def predict(self, source_seq, source_lengths):\n",
        "    \"\"\"\n",
        "    Generates translations for the given source sequences.\n",
        "\n",
        "    Args:\n",
        "      source_seq (Tensor): Source sequences (batch_size, source_sequence_length).\n",
        "      source_lengths (Tensor): Lengths of source sequences before padding (batch_size,).\n",
        "\n",
        "    Returns:\n",
        "      predicted_tokens (Tensor): Generated token indices (batch_size, max_length).\n",
        "    \"\"\"\n",
        "    batch_size = source_seq.size(0)\n",
        "\n",
        "    ####################################################################################\n",
        "    # TODO: Implement the RNNTranslator prediction function. Should be similar with\n",
        "    # the forward pass but not the same. Keep notes that you should return predicted\n",
        "    # tokens and not logits.\n",
        "    ####################################################################################\n",
        "\n",
        "    # Your implementation code\n",
        "\n",
        "    final_hidden, final_cell = self.encoder(source_seq, source_lengths)\n",
        "\n",
        "    # 2. Decoder에 hidden state와 cell state를 넘겨줌, target sequence 없이 prediction 수행\n",
        "    outputs, _ = self.decoder(final_hidden, initial_cell = final_cell) # outputs, hidden_state, 이때 outputs = raw scores with size (batch_size, target_sequence_length, vocab_size)\n",
        "    # print(outputs) -> (체크) 이 값 이상해\n",
        "    # print(outputs.size()) =  [64, 12, 4771]\n",
        "    \n",
        "    predicted_tokens = torch.argmax(outputs, dim=-1)[:, :-1] # slicing to remove the last token\n",
        "    sos_token = torch.tensor([BOS_TOKEN] * batch_size).to(source_seq.device).unsqueeze(1)\n",
        "    predicted_tokens = torch.cat((sos_token, predicted_tokens), dim=1) # add sos token to the shift-righted prediction\n",
        "\n",
        "    # put PAD token after the first EOS token\n",
        "    for i in range(batch_size):\n",
        "      eos_index = (predicted_tokens[i] == EOS_TOKEN).nonzero(as_tuple=True)[0]\n",
        "      if eos_index.numel() > 0:  # if EOS token is found\n",
        "        eos_index = eos_index[0].item()  # get the first occurrence\n",
        "        predicted_tokens[i, eos_index + 1:] = PAD_TOKEN  # pad after EOS\n",
        "      else:  # if no EOS token is found\n",
        "        predicted_tokens[i, -1] = EOS_TOKEN  # put EOS token at the end\n",
        "\n",
        "    ####################################################################################\n",
        "    # END OF YOUR CODE\n",
        "    ####################################################################################\n",
        "\n",
        "    return predicted_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD26pZiATmB2"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq8YDahTTmB3"
      },
      "source": [
        "### Tensorboard\n",
        "\n",
        "Before run the training and evaluation, run the Tensorboard to log your loss and other metrics from training and evaluation. Use this to track your experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2qYWVaGHTmB5"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorboard # install tensorboard first if not installed\n",
        "# !tensorboard --logdir=runs # run the tensorboard. you can run it directly if you use Colab, but if you run it locally, then run this command on the terminal/command line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiI2ydoPTmB7"
      },
      "source": [
        "### Accuracy Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAtR5T5tTmB7"
      },
      "source": [
        "Below is the function to compute the accuracy given the tensor of predicted tokens and target data. The accuracy score for a batch is average of accuracy scores from all batch data.\n",
        "\n",
        "You must make sure all the inputs are in the same device. One of the option is to move all of them to CPU first rather than compute it on GPU, before you put them into this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aW-2TBDlTmB8"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(predicted_tokens, target_seq, target_len):\n",
        "    # Make sure pred tokens and target seq has the same shape\n",
        "    assert predicted_tokens.shape == target_seq.shape\n",
        "\n",
        "    # Create a mask for valid tokens (excluding padding)\n",
        "    max_len = target_seq.size(1)\n",
        "    batch_size = target_seq.size(0)\n",
        "\n",
        "    # Create a mask tensor where 1 indicates valid tokens and 0 indicates padding\n",
        "    mask = torch.arange(max_len).expand(batch_size, max_len).to(target_len.device) < (target_len - 1).unsqueeze(1)\n",
        "\n",
        "    # Compare predicted tokens with true tokens\n",
        "    correct_predictions = (predicted_tokens == target_seq) & mask\n",
        "\n",
        "    # Calculate the number of correct predictions and sequence lengths for each sample\n",
        "    correct_counts = correct_predictions.sum(dim=1).float()\n",
        "    sequence_lengths = mask.sum(dim=1).float()\n",
        "\n",
        "    # Calculate accuracy for each sequence\n",
        "    batch_accuracy = correct_counts / sequence_lengths\n",
        "\n",
        "    return batch_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFoXiGd6TmB9"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyXHNWsvTmB-"
      },
      "source": [
        "Now you will implement a training loop for a Translator model using PyTorch. This training loop will be used not only for the RNNTranslator, but also re-used for the TransformerTranslator which you will be implemented after this section. The training loop will:\n",
        "- Iterate over the training and validation datasets, train you model and check it through the validation dataset.\n",
        "- Compute both training and validation loss while ignoring padding tokens, and perform backpropagation and optimization. Some notes:\n",
        "    - Use `Adam` as the optimizer.\n",
        "    - Depending on how you manipulate the output tensor matrices, you can either use `CrossEntropyLoss` of `NLLLoss` for the loss function.\n",
        "    - The `BOS_token` and `PAD_token` must be ignored when calculating the loss. Therefore, you must only calculate the loss through the valid length - 1 (excluding), or until the `EOS_token` of the input sequence.\n",
        "    - The final calculated loss for each data is the average loss from all token losses in the sequence. Sum it for all training data to obtain the total training loss for 1 epoch, which should be logged.\n",
        "- Log training and validation loss and accuracy to TensorBoard for monitoring. Use arg `run_name` to track your run.\n",
        "\n",
        "Instructions:\n",
        "- Complete the `train_model()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HrdRZvunTmB_"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_iter, val_iter, lr, epochs, device, run_name=\"experiment-1\"):\n",
        "    \"\"\"\n",
        "    Trains a Translator model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Translator model to train.\n",
        "        train_iter (DataLoader): Training data iterator.\n",
        "        val_iter (DataLoader): Validation data iterator.\n",
        "        lr (float): Learning rate.\n",
        "        epochs (int): Number of epochs to train.\n",
        "        device (device): Device used to train.\n",
        "        run_name (str): Run name to log on Tensorboard.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "\n",
        "    ####################################################################################\n",
        "    # TODO: Setup the optimizer and loss function based on the explanation above\n",
        "    ####################################################################################\n",
        "\n",
        "    # Your implementation code\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index= PAD_TOKEN)\n",
        "\n",
        "    ####################################################################################\n",
        "    # END OF YOUR CODE\n",
        "    ####################################################################################\n",
        "\n",
        "    # Set up TensorBoard writer\n",
        "    writer = SummaryWriter(log_dir=f'runs/{run_name}')\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_batches = 0\n",
        "\n",
        "        # Iterate over training batches\n",
        "        for _, train_data in tqdm(enumerate(train_iter), desc=f\"Epoch {epoch} Training\"):\n",
        "            ####################################################################################\n",
        "            # TODO: Implement the model training for one batch. Here you must\n",
        "            # 1. Do the forward pass through the model\n",
        "            # 2. Update the gradients through backpropagation\n",
        "            # 3. Compute the training loss, according to the explanation above\n",
        "            ####################################################################################\n",
        "\n",
        "            # Your implementation code\n",
        "            source_seq, source_len, target_seq, target_len = train_data\n",
        "\n",
        "            source_seq = source_seq.to(device)\n",
        "            target_seq = target_seq.to(device)\n",
        "            source_len = source_len.to(device)\n",
        "            target_len = target_len.to(device)\n",
        "\n",
        "            # print(\"target\")\n",
        "            # print(target_seq[0, :])\n",
        "            # print(\"predict\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # logits = model(source_seq, source_len, target_seq, target_len) # forward pass (돌아갔던 코드)\n",
        "            logits = model(source_seq, source_len, target_seq, target_len)\n",
        "            # print(logits.argmax(dim=-1)[0,:])\n",
        "\n",
        "\n",
        "            logits = logits[:, :-1, :]  # ignore the first token prediction\n",
        "\n",
        "            #loss = loss_fn(logits.view(-1, logits.size(-1)), target_seq[:, 1:].contiguous().view(-1)) # loss 계산\n",
        "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), target_seq[:, 1:].contiguous().reshape(-1))  # view 대신 reshape 사용\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_batches += 1\n",
        "\n",
        "            ####################################################################################\n",
        "            # END OF YOUR CODE\n",
        "            ####################################################################################\n",
        "\n",
        "        average_train_loss = total_train_loss / total_train_batches\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        total_val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _, val_data in tqdm(enumerate(val_iter), desc=f\"Epoch {epoch} Validation\"):\n",
        "                ####################################################################################\n",
        "                # TODO: Implement the model validation for one batch. Here you must\n",
        "                # 1. Do the forward pass through the model\n",
        "                # 2. Compute the validation loss, according to the explanation above\n",
        "                ####################################################################################\n",
        "\n",
        "                # Your implementation code\n",
        "                source_seq, source_len, target_seq, target_len = val_data\n",
        "                source_seq = source_seq.to(device)\n",
        "                target_seq = target_seq.to(device)\n",
        "                source_len = source_len.to(device)\n",
        "                target_len = target_len.to(device)\n",
        "\n",
        "                logits = model(source_seq, source_len, target_seq, target_len)\n",
        "                logits = logits[:, :-1, :]  # 보류\n",
        "\n",
        "                # loss = loss_fn(logits.view(-1, logits.size(-1)), target_seq[:, 1:].contiguous().view(-1))\n",
        "                loss = loss_fn(logits.reshape(-1, logits.size(-1)), target_seq[:, 1:].contiguous().reshape(-1))  # view 대신 reshape 사용\n",
        "\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "                total_val_batches += 1\n",
        "                \n",
        "        \n",
        "\n",
        "                ####################################################################################\n",
        "                # END OF YOUR CODE\n",
        "                ####################################################################################\n",
        "\n",
        "        average_val_loss = total_val_loss / total_val_batches\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        writer.add_scalar('Loss/Train', average_train_loss, epoch)\n",
        "        writer.add_scalar('Loss/Validation', average_val_loss, epoch)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\"\"Epoch {epoch}/{epochs}\n",
        "Train Loss: {average_train_loss:.4f}\n",
        "Validation Loss: {average_val_loss:.4f}\\n\"\"\")\n",
        "\n",
        "    # Close the TensorBoard writer\n",
        "    writer.close()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKuEI39FTmCA"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwZ_ND3JTmCB"
      },
      "source": [
        "Below you should implement the evaluation loop. Same with calculating loss, the `BOS_token` and `PAD_token` must also be ignored when calculating the accuracy. Given the model and a dataloader, use the model to predict the data and compute the accuracy using the provided `compute_accuracy` function.\n",
        "\n",
        "Instructions:\n",
        "- Complete the `evaluate_model()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3t_v_Q0rTmCB"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, eval_iter, device):\n",
        "    \"\"\"\n",
        "    Runs prediction and evaluates the a Translator model on the given data iterator.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained a Translator model.\n",
        "        eval_iter (DataLoader): Data iterator for evaluation data.\n",
        "        device (torch.device): Device to run the model on.\n",
        "\n",
        "    Returns:\n",
        "        overall_accuracy (float): Overall accuracy of the model on the evaluation data.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_accuracies = []\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, eval_data in enumerate(eval_iter):\n",
        "            ####################################################################################\n",
        "            # TODO: Implement the model evaluation for one batch. Here you must\n",
        "            # 1. Do the model prediction\n",
        "            # 2. Use the provided compute_accuracy() function to get batch accuracy\n",
        "            ####################################################################################\n",
        "\n",
        "            # Your implementation code\n",
        "            \n",
        "            source_seq, source_len, target_seq, target_len = eval_data\n",
        "            source_seq = source_seq.to(device)\n",
        "            target_seq = target_seq.to(device)\n",
        "            source_len = source_len.to(device)\n",
        "            target_len = target_len.to(device)\n",
        "\n",
        "            predicted_tokens = model.predict(source_seq, source_len)\n",
        "            # print(\"predict 값\")\n",
        "            # print(predicted_tokens[0, :]) # -> 체크(이 값 이상해)\n",
        "            # print(\"target 값\")\n",
        "            # print(target_seq[0, :])\n",
        "\n",
        "            batch_accuracy = compute_accuracy(predicted_tokens, target_seq, target_len)\n",
        "            all_accuracies.append(batch_accuracy)\n",
        "\n",
        "            ####################################################################################\n",
        "            # END OF YOUR CODE\n",
        "            ####################################################################################\n",
        "\n",
        "    overall_accuracy = torch.cat(all_accuracies).mean()\n",
        "\n",
        "    return overall_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ice5Zt3BTmCD"
      },
      "source": [
        "## Set Hyperparameters, Build Vocab, Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dGDDWE9TmCE"
      },
      "source": [
        "Now we are ready to train our model. Let's setup the hyperparameters and re-build and re-load the vocab and the dataloader.\n",
        "\n",
        "**IMPORTANT NOTES**\n",
        "\n",
        "Since one of the parameters of `load_data_nmt` is `batch_size`, you should make sure to re-load the dataset if you change the batch size, especially when experimenting with hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fshLyfCTTmCG"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 1e-3\n",
        "epochs = 10\n",
        "embed_dim = 64\n",
        "hidden_dim = 512\n",
        "\n",
        "src, tgt = split_source_target(raw_text, MAX_LEN)\n",
        "\n",
        "eng_vocab = Lang('eng', src)\n",
        "fra_vocab = Lang('fra', tgt)\n",
        "\n",
        "train_iter, val_iter, test_iter = load_data_nmt(src, tgt, eng_vocab, fra_vocab, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJv5k5hZTmCH"
      },
      "source": [
        "## RNNTranslator Training & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq5r1LsOTmCJ"
      },
      "source": [
        "We will train your implementation of `RNNTranslator` with `VanillaRNN` as both Encoder and Decoder. If your all implementation above was right, then there should be no problem to run the code below. You also should inspect the training loss and the validation loss, whether there are something not right with your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vL1mKlL0TmCM"
      },
      "outputs": [],
      "source": [
        "enc_net = VanillaRNN(embed_dim, hidden_dim)\n",
        "dec_net = VanillaRNN(embed_dim, hidden_dim)\n",
        "rnn_net = RNNTranslator(eng_vocab.vocab_size, fra_vocab.vocab_size, embed_dim, hidden_dim, enc_net, dec_net)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "28QsdkUKTmCN",
        "outputId": "949b346b-d93b-406c-cf62-46887190bab1"
      },
      "outputs": [],
      "source": [
        "trained_rnn_net = train_model(rnn_net, train_iter, val_iter, lr, epochs, DEVICE, run_name=\"VanillaRNN-VanillaRNN-baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xuOirxOTmCP",
        "outputId": "d1d25eed-3d2f-4799-9598-472eeb328f75"
      },
      "outputs": [],
      "source": [
        "acc = evaluate_model(trained_rnn_net, val_iter, DEVICE)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yjiDuAUTmCP"
      },
      "source": [
        "### Exploring RNNTranslator Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqcX0KWXTmCQ"
      },
      "source": [
        "In the code block below, you must also run the training for both LSTM and GRU (where Encoder and Decoder has the same RNN variants). It is optional for you to explore if the Encoder and Decoder has different variants, but it is really helpful especially if you suspect something wrong with your code.\n",
        "\n",
        "Instructions:\n",
        "- (Mandatory) Run the training & evaluation for both LSTM and GRU where Encoder and Decoder has the same RNN variants. You should run the evaluation on the `train_iter` and `val_iter`, but **NOT** on `test_iter`\n",
        "- (Optional) Explore RNNTranslator when it has different RNN variant between Encoder and Decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qV697S6vTmCQ"
      },
      "outputs": [],
      "source": [
        "# Your code below\n",
        "enc_net = LSTM(embed_dim, hidden_dim)\n",
        "dec_net = LSTM(embed_dim, hidden_dim)\n",
        "lstm_rnn_net = RNNTranslator(eng_vocab.vocab_size, fra_vocab.vocab_size, embed_dim, hidden_dim, enc_net, dec_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_0QQIQXdNAc",
        "outputId": "8730e6d8-562e-4f82-d384-88cadde7ea16"
      },
      "outputs": [],
      "source": [
        "lstm_trained_rnn_net = train_model(lstm_rnn_net, train_iter, val_iter, lr, epochs, DEVICE, run_name=\"LSTM-LSTM-baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x6XlAdidU49",
        "outputId": "6259ef80-8016-4f21-bc8a-76c2ee956164"
      },
      "outputs": [],
      "source": [
        "lstm_acc = evaluate_model(lstm_trained_rnn_net, val_iter, DEVICE)\n",
        "print(lstm_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "55-8Z28Lde2Y"
      },
      "outputs": [],
      "source": [
        "enc_net = GRU(embed_dim, hidden_dim)\n",
        "dec_net = GRU(embed_dim, hidden_dim)\n",
        "gru_rnn_net = RNNTranslator(eng_vocab.vocab_size, fra_vocab.vocab_size, embed_dim, hidden_dim, enc_net, dec_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT5Hos7kdkBv",
        "outputId": "4374b314-3747-4049-b198-ca8c8c9a9ccd"
      },
      "outputs": [],
      "source": [
        "gru_trained_rnn_net = train_model(gru_rnn_net, train_iter, val_iter, lr, epochs, DEVICE, run_name=\"GRU-GRU-baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI2Ti3Brdmw9",
        "outputId": "81f21056-ecbd-4a34-fd83-dc22dbf89397"
      },
      "outputs": [],
      "source": [
        "gru_acc = evaluate_model(gru_trained_rnn_net, val_iter, DEVICE)\n",
        "print(gru_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN9B_X3TTmCR"
      },
      "source": [
        "## Transformer From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZrID5XVTmCT"
      },
      "source": [
        "The Transformer architecture, introduced in the \"Attention Is All You Need\" paper, revolutionized natural language processing by eliminating the need for recurrent neural networks while achieving superior performance.\n",
        "\n",
        "In this section, you'll build each component of the Transformer, through the multi-head attention mechanism, the feed-forward networks, and finally the encoder and decoder.\n",
        "\n",
        "Key aspects you'll focus on:\n",
        "- Multi-head attention mechanism\n",
        "- Feed-forward neural networks + LayerNorm + Residual Connections\n",
        "- Self-attention and cross-attention\n",
        "- The overall encoder-decoder architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-pZOOkmTmCV"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsKg1I1oTmCW"
      },
      "source": [
        "Positional Encoding is crucial in Transformer architectures because the model itself contains no inherent understanding of word order. To inject information about the relative or absolute position of tokens in a sequence, we add positional encodings to the input embeddings.\n",
        "\n",
        "The positional encoding for a position `pos `and dimension `i` is calculated using the following equations:\n",
        "\n",
        "For even indices (i = 0, 2, 4, ...):\n",
        "```\n",
        "PE(pos, i) = sin(pos / 10000^(i/d_model))\n",
        "```\n",
        "\n",
        "For odd indices (i = 1, 3, 5, ...):\n",
        "```\n",
        "PE(pos, i) = cos(pos / 10000^(i/d_model))\n",
        "```\n",
        "\n",
        "where:\n",
        "- `pos` is the position of the token in the sequence\n",
        "- `i` is the dimension in the embedding vector\n",
        "- `d_model` is the dimensionality of the model's embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LWzizk7ATmCX"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        \"\"\"\n",
        "        Initialize the PositionalEncoding layer.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the model embeddings\n",
        "            dropout (float): Dropout probability (default: 0.1)\n",
        "            max_len (int): Maximum sequence length to pre-compute (default: 5000)\n",
        "\n",
        "        Creates:\n",
        "            pe (Tensor): Positional encoding buffer of shape (1, max_len, d_model)\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model/2)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add positional encoding to input embeddings.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input embeddings of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Embeddings with positional encoding added and dropout applied,\n",
        "                   shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skm8unHETmCY"
      },
      "source": [
        "### Masked Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vS6KEJTmCZ"
      },
      "source": [
        "Masked Softmax is a crucial component in Transformer architectures, used to prevent certain attention connections. The regular softmax function is defined as:\n",
        "```\n",
        "softmax(x_i) = exp(x_i) / Σ_j exp(x_j)\n",
        "```\n",
        "\n",
        "Masked softmax modifies this by setting certain values to negative infinity before applying softmax:\n",
        "```\n",
        "masked_softmax(x_i) = 0 if i is masked\n",
        "                                            exp(x_i) / Σ_j exp(x_j) otherwise\n",
        "```\n",
        "        \n",
        "**Types of Masks**\n",
        "\n",
        "1. Source Mask (Encoder Self-Attention & Decoder Cross-Attention):\n",
        "    - Shape: (B, 1, m), where B is batch size and m is key sequence length\n",
        "    - Used to mask padding tokens in the source sequence\n",
        "    - Prevents attention to padding tokens\n",
        "2. Target Mask (Decoder Self-Attention):\n",
        "    - Shape: (1, n, n), where n is query/target sequence length\n",
        "    - Used to prevent attention to future tokens\n",
        "    - Creates causal/autoregressive attention pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "siu-THbdTmCb"
      },
      "outputs": [],
      "source": [
        "def masked_softmax(vector, mask, dim=-1):\n",
        "    \"\"\"\n",
        "    Performs masked softmax. Works for both source and target masks.\n",
        "\n",
        "    Args:\n",
        "        vector: Tensor of shape (B, num_heads, n, m) where B is batch size,\n",
        "               num_heads is num_heads, n is num_queries, and m is num_keys\n",
        "        mask: Boolean mask that can be either:\n",
        "              - Source mask of shape (B, 1, m) for encoder-decoder attention\n",
        "              - Target mask of shape (1, n, n) for self-attention in decoder\n",
        "        dim: Dimension to apply softmax over, default is -1\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape (B, n, m) with masked softmax probabilities\n",
        "    \"\"\"\n",
        "    # Adjusting mask shape and type if needed\n",
        "    if mask.dim() == 3:  # Need to add num_heads dimension\n",
        "        if mask.size(0) == 1:  # Target mask case: (1, n, n) -> (1, 1, n, n)\n",
        "            mask = mask.unsqueeze(1)\n",
        "        else:  # Source mask case: (B, 1, m) -> (B, 1, 1, m)\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "    # Convert boolean mask to float mask where True is -inf and False is 0\n",
        "    float_mask = mask.float().masked_fill(mask, float('-inf'))\n",
        "\n",
        "    # Add the mask to the vector\n",
        "    masked_vector = vector + float_mask\n",
        "\n",
        "    return F.softmax(masked_vector, dim=dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trk4MQa6TmCc"
      },
      "source": [
        "### Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzqHXX9UTmCd"
      },
      "source": [
        "Scaled Dot Product Attention computes attention weights using queries, keys, and values. The formula is:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\alpha(\\mathbf{q}, \\mathbf{k}) = \\text{softmax}(\\mathbf{Q} \\mathbf{K}^T / \\sqrt{d_k})V\n",
        "\\end{align*}\n",
        "$$\n",
        "where:\n",
        "- Q is the query matrix\n",
        "- K is the key matrix\n",
        "- V is the value matrix\n",
        "- d_k is the dimension of the keys\n",
        "- √d_k is the scaling factor\n",
        "\n",
        "The scaling factor √d_k prevents the dot products from growing too large in magnitude, leading to extremely small gradients in the softmax function.\n",
        "\n",
        "<div>\n",
        "    <img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"600\"/>\n",
        "    </div>\n",
        "Image source: http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\n",
        "\n",
        "Instructions:\n",
        "- Complete the `ScaledDotProductAttention` forward() function. In this case, use the provided `masked_softmax` instead of built-in softmax. Implement the function without using any loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iaFJaXBbTmCe"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "            super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "            query: tensor of size (B, num_heads, n, d)\n",
        "            key: tensor of size (B, num_heads, m, d)\n",
        "            value: tensor of size (B, num_heads, m, dim_v)\n",
        "            mask: Boolean mask that can be either:\n",
        "                    - Source mask of shape (B, 1, m) for encoder-decoder attention\n",
        "                    - Target mask of shape (1, n, n) for self-attention in decoder\n",
        "\n",
        "            B is the batch_size, num_heads is the number of attn head, n is the number of queries, m is the number of keys,\n",
        "            d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
        "\n",
        "        Outputs:\n",
        "            attention: tensor of size (B, num_heads, n, dim_v), weighted sum of values\n",
        "        \"\"\"\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the ScaledDotProductAttention forward pass according to equation.\n",
        "        # Use `masked_softmax()` instead of pytorch softmax. Do not use any loop.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        d = query.size(-1)  # feature dimension of the query\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d) # scores = (B, num_heads, n, m)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_weights = masked_softmax(scores, mask, dim = -1) # (B, num_heads, n, m)\n",
        "        else:\n",
        "            attention_weights = F.softmax(scores, dim=-1) # (B, num_heads, n, m)\n",
        "\n",
        "        attention = torch.matmul(attention_weights, value) # (B, num_heads, n, dim_v)\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "        return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmHUl7aFTmCg"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "Multi-Head Attention allows the model to jointly attend to information from different representation subspaces. The mathematical formulation is:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{MultiHead(Q, K, V) = Concat}(\\text{head}_1, ..., \\text{head}_h)\\text{W}^O \\\\\n",
        "&\\text{where head}_i = \\text{Attention}(\\text{QW}^\\text{Q}_i, \\text{KW}^\\text{K}_i, \\text{VW}^\\text{V}_i)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The projections are parameter matrices:\n",
        "- $W^Q_i ∈ ℝ^(d_{model} × d_k)$\n",
        "- $W^K_i ∈ ℝ^(d_{model} × d_k)$\n",
        "- $W^V_i ∈ ℝ^(d_{model} × d_v)$\n",
        "- $W^O ∈ ℝ^(d_v × d_{model})$\n",
        "\n",
        "In our case, since we pad both source and target sequence, therefore: $d_k = d_v = d_{model}$\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" width=\"400\"/>\n",
        "    </div>\n",
        "Image source: https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\n",
        "\n",
        "Instructions:\n",
        "- Complete the `MultiHeadAttention` init() and forward() function. Use the `ScaledDotProductAttention` you have implemented. To expand the attention head into `num_heads`, you are **prohibited** to use any loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NqZObW_UTmCh"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "            \"\"\"\n",
        "            Initialize the Multi-Head Attention module.\n",
        "\n",
        "                Args:\n",
        "                        d_model (int): The dimension of the model (input and output)\n",
        "                        num_heads (int): The number of attention heads\n",
        "\n",
        "                Attributes:\n",
        "                        d_k (int): The dimension of each head (d_model / num_heads)\n",
        "                        linear_q (nn.Linear): Linear projection for queries\n",
        "                        linear_k (nn.Linear): Linear projection for keys\n",
        "                        linear_v (nn.Linear): Linear projection for values\n",
        "                        linear_out (nn.Linear): Final linear projection\n",
        "                        dot_product (ScaledDotProductAttention): Attention mechanism\n",
        "                \"\"\"\n",
        "            super(MultiHeadAttention, self).__init__()\n",
        "            assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "            ####################################################################################\n",
        "            # TODO: Implement the MultiHeadAttention constructor. Think of all the attributes\n",
        "            # that you need in order to run the forward pass.\n",
        "            ####################################################################################\n",
        "\n",
        "            # Your implementation code\n",
        "            self.d_model = d_model\n",
        "            self.num_heads = num_heads\n",
        "            self.d_k = d_model // num_heads\n",
        "\n",
        "            # self.linear_q = nn.Linear(d_model, d_model)\n",
        "            # self.linear_k = nn.Linear(d_model, d_model)\n",
        "            # self.linear_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "            # self.linear_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "            self.linear_q = Linear(d_model, d_model)\n",
        "            self.linear_k = Linear(d_model, d_model)\n",
        "            self.linear_v = Linear(d_model, d_model)\n",
        "\n",
        "            self.linear_out = Linear(d_model, d_model)\n",
        "\n",
        "            self.dot_product = ScaledDotProductAttention()\n",
        "\n",
        "            ####################################################################################\n",
        "            # END OF YOUR CODE\n",
        "            ####################################################################################\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "            \"\"\"\n",
        "                Compute multi-head attention.\n",
        "\n",
        "                Args:\n",
        "                        query: Tensor of shape (batch_size, seq_len_q, d_model)\n",
        "                        key: Tensor of shape (batch_size, seq_len_k, d_model)\n",
        "                        value: Tensor of shape (batch_size, seq_len_v, d_model)\n",
        "                        mask: Optional boolean mask\n",
        "\n",
        "                Returns:\n",
        "                        output: Tensor of shape (batch_size, seq_len_q, d_model)\n",
        "                \"\"\"\n",
        "            batch_size = query.size(0)\n",
        "\n",
        "            ####################################################################################\n",
        "            # TODO: Implement the MultiHeadAttention forward pass. Implement it based on the eq.\n",
        "            # and image. Do not use any loop.\n",
        "            ####################################################################################\n",
        "\n",
        "            # Your implementation code\n",
        "            q = self.linear_q(query)\n",
        "            k = self.linear_k(key)\n",
        "            v = self.linear_v(value)\n",
        "\n",
        "            q = q.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) # (B, seq_len_q, num_heads, d_k)\n",
        "            k = k.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "            v = v.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "            # Attention output shape : mask shape 4D로 만들기\n",
        "            if mask is not None:\n",
        "                if mask.dim() == 3:\n",
        "                    mask = mask.unsqueeze(1)\n",
        "                elif mask.dim() == 2:\n",
        "                    mask = mask.unsqueeze(1).unsqueeze(1)\n",
        "            attention = self.dot_product(q, k, v, mask = mask) # (B, n, seq_len, dim_k)\n",
        "\n",
        "            attention = attention.transpose(1,2).reshape(batch_size, -1, self.d_model) # (B, seq_len, d_model)\n",
        "\n",
        "            output = self.linear_out(attention) # (B, seq_len, d_mdoel)\n",
        "\n",
        "            ####################################################################################\n",
        "            # END OF YOUR CODE\n",
        "            ####################################################################################\n",
        "\n",
        "            return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZd6Il5GTmCj"
      },
      "source": [
        "### Positionwise FFNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UjBsIK9TmCk"
      },
      "source": [
        "Positionwise FFNN is the forward pass consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "The mathematical equation for this layer can be described as:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{FFNN(}x\\text{) = Linear}_2\\text{(ReLU(Linear}_1\\text{(} x \\text{)))}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\text{Linear}_1\\text{(}x\\text{) = } W_1x + b_1$ projects from `d_model` to `d_ff`.\n",
        "- $\\text{Linear}_2\\text{(}x\\text{) = } W_2x + b_2$ projects back from `d_ff` to `d_model`.\n",
        "- ReLU applies a non-linear activation function.\n",
        "\n",
        "Instructions:\n",
        "- Complete the `PositionwiseFeedForward` init() and forward() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5M7O-ieETmCk"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        \"\"\"\n",
        "        Initializes the Positionwise FeedForward layer.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The input dimension (same as model's hidden size).\n",
        "            d_ff (int): The dimension of the intermediate feedforward layer (usually larger than d_model).\n",
        "            dropout (float, optional): Dropout rate for regularization, defaults to 0.1.\n",
        "\n",
        "        Returns:\n",
        "            None: Initializes the layers and dropout for feedforward pass.\n",
        "        \"\"\"\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the PositionwiseFeedForward constructor.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        #self.linear1 = nn.Linear(d_model, d_ff)\n",
        "\n",
        "        self.linear1 = Linear(d_model, d_ff)\n",
        "        #self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.linear2 = Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Positionwise FeedForward layer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, sequence_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (batch_size, sequence_length, d_model), same as input dimension.\n",
        "                    The output goes through two linear transformations with a ReLU activation and dropout applied\n",
        "                    between them.\n",
        "        \"\"\"\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the PositionwiseFeedForward forward pass. Follow the equation.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJwGyZzTmCl"
      },
      "source": [
        "### Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6QcTkufTmCm"
      },
      "source": [
        "The `TransformerEncoderLayer` is a building block of the Transformer model. Each layer consists of two key sub-layers:\n",
        "\n",
        "- Multi-head self-attention\n",
        "- Positionwise feedforward (FFNN)\n",
        "\n",
        "Each sub-layer is followed by Layer Normalization and a Residual Connection. Residual connections help with gradient flow during training, and layer normalization helps stabilize learning.\n",
        "\n",
        "The equations for `TransformerEncoderLayer` are:\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{Self-Attention(}x\\text{) = MultiHead(Q}_x\\text{, K}_x\\text{, V}_x) \\\\\n",
        "&\\text{Output: }x_1 = \\text{LayerNorm}(x + \\text{Self-Attention}(x)) \\\\\n",
        "&\\text{Positionwise FFNN: }x_2 = \\text{LayerNorm}(x_1 + \\text{FFNN}(x_1))\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "The `TransformerEncoder` is a stack of `N` `TransformerEncoderLayer` layers. The encoder uses embeddings to represent words and adds positional information since Transformers lack an inherent sense of word order.\n",
        "\n",
        "The input goes through the following steps:\n",
        "- Embedding: Each token is mapped to a dense vector representation.\n",
        "- Positional Encoding: Positional information is added to the embeddings to account for sequence order.\n",
        "- Stack of Encoder Layers: The input is passed through N encoder layers, each consisting of self-attention and feed-forward sub-layers.\n",
        "\n",
        "Instructions:\n",
        "- Complete both the `TransformerEncoderLayer` and `TransformerEncoder` init() and forward() function.\n",
        "- You are allowed to use `nn.LayerNorm` for layer normalization, also allowed to use loop when implementing `N` stacks of `TransformerEncoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "XSLS_p6bTmCn"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        \"\"\"\n",
        "        Initializes a single Transformer Encoder layer.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The input and output dimension size.\n",
        "            num_heads (int): The number of attention heads in multi-head attention.\n",
        "            d_ff (int): The dimension of the feed-forward network's hidden layer.\n",
        "\n",
        "        Returns:\n",
        "            None: Sets up self-attention, feed-forward, and normalization layers.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the TransformerEncoderLayer constructor. Think of all building\n",
        "        # blocks you need here. You are able to use `nn.LayerNorm` here. Use your\n",
        "        # implemented `MultiHeadAttention` and `PositionwiseFeedForward`.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for a single encoder layer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, sequence_length, d_model).\n",
        "            src_mask (Tensor, optional): Mask to avoid attending to certain positions, shape (batch_size, 1, seq_len).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (batch_size, sequence_length, d_model) after self-attention and FFNN.\n",
        "        \"\"\"\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the TransformerEncoderLayer forward pass. Refer to the equation\n",
        "        # and the image to be clear on how to implement it.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        attention_output = self.self_attention(x, x, x, mask = src_mask) # self attention이라 k, q, v = x\n",
        "        x = x + self.dropout1(attention_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout2(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, num_heads, d_ff):\n",
        "        \"\"\"\n",
        "        Initializes the Transformer Encoder consisting of multiple encoder layers.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary.\n",
        "            d_model (int): The dimension of the input/output embeddings.\n",
        "            N (int): Number of encoder layers (stacked TransformerEncoderLayers).\n",
        "            num_heads (int): Number of attention heads in each layer's multi-head attention.\n",
        "            d_ff (int): The dimension of the feed-forward network's hidden layer.\n",
        "\n",
        "        Returns:\n",
        "            None: Initializes the embedding, positional encoding, and encoder layers.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the TransformerEncoder constructor. Think of all building\n",
        "        # blocks you need here. You are able to use `nn.Embedding` and `nn.ModuleList` here,\n",
        "        # and use the provided `PositionalEncoding` class.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff) for _ in range(N)])\n",
        "        \n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the full Transformer Encoder.\n",
        "\n",
        "        Args:\n",
        "            src (Tensor): Input source sequence tensor of shape (batch_size, sequence_length).\n",
        "            src_mask (Tensor, optional): Source mask tensor of shape (batch_size, 1, seq_len), defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (batch_size, sequence_length, d_model), the encoded representation of the input.\n",
        "        \"\"\"\n",
        "        ####################################################################################\n",
        "        # TODO: Implement the TransformerEncoder forward pass. Refer to the image to be\n",
        "        # clear. You allowed to use loop here.\n",
        "        ####################################################################################\n",
        "\n",
        "        # Your implementation code\n",
        "        x = self.embedding(src)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "          x = layer(x, src_mask)\n",
        "        \n",
        "\n",
        "        ####################################################################################\n",
        "        # END OF YOUR CODE\n",
        "        ####################################################################################\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVSGaKXnTmCo"
      },
      "source": [
        "### Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpsDxn7uTmCo"
      },
      "source": [
        "The `TransformerDecoderLayer` is a core component of the Transformer decoder. Each layer consists of three main sub-layers:\n",
        "- Self-attention: The decoder attends to its own output sequence, ensuring that each position in the sequence can capture dependencies from earlier positions.\n",
        "- Cross-attention (Encoder-Decoder Attention): The decoder attends to the encoder's output (referred to as the \"memory\"), allowing the decoder to incorporate information from the source sequence.\n",
        "- Feedforward Network (FFN)\n",
        "\n",
        "The equations for `TransformerDecoderLayer` are:\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{Self-Attention(}x\\text{) = MultiHead(Q}_x\\text{, K}_x\\text{, V}_x) \\\\\n",
        "&\\text{Output: }x_1 = \\text{LayerNorm}(x + \\text{Self-Attention}(x)) \\\\\n",
        "&\\text{Cross-Attention(}x\\text{, src) = MultiHead(Q}_x\\text{, K}_{\\text{src}}\\text{, V}_{\\text{src}}) \\\\\n",
        "&\\text{Output: }x_2 = \\text{LayerNorm}(x + \\text{Cross-Attention}(x)) \\\\\n",
        "&\\text{Positionwise FFNN: }x_3 = \\text{LayerNorm}(x_2 + \\text{FFNN}(x_2)) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The `TransformerDecoder` is composed of a stack of `N` `TransformerDecoderLayers`. It is responsible for generating the target sequence by attending to both its own previous outputs (self-attention) and the encoder's output (cross-attention). It also includes an embedding layer to convert target token indices into dense vectors and positional encoding to add order information.\n",
        "\n",
        "The input goes through similar steps to `TransformerEncoder`: Embedding -> Positional Encoding -> Stack of N `TransformerDecoderLayers`\n",
        "\n",
        "Instructions:\n",
        "- Complete both the `TransformerDecoderLayer` and `TransformerDecoder` init() and forward() function.\n",
        "- You are allowed to use `nn.LayerNorm` for layer normalization, also allowed to use loop when implementing `N` stacks of `TransformerDecoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5LT_6-9CTmCp"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff):\n",
        "      \"\"\"\n",
        "      Initializes a single Transformer Decoder layer.\n",
        "\n",
        "      Args:\n",
        "          d_model (int): The dimension of the input/output embeddings.\n",
        "          num_heads (int): Number of attention heads for self-attention and cross-attention.\n",
        "          d_ff (int): Dimension of the feed-forward network's hidden layer.\n",
        "\n",
        "      Returns:\n",
        "          None: Sets up self-attention, cross-attention, feed-forward, and normalization layers.\n",
        "      \"\"\"\n",
        "      super(TransformerDecoderLayer, self).__init__()\n",
        "      ####################################################################################\n",
        "      # TODO: Implement the TransformerDecoderLayer constructor. Think of all building\n",
        "      # blocks you need here. You are able to use `nn.LayerNorm` and `nn.ModuleList` here,\n",
        "      # and use your implemented `MultiHeadAttention` and `PositionwiseFeedForward` class.\n",
        "      ####################################################################################\n",
        "\n",
        "      # Your implementation code\n",
        "      self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "      self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
        "      self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "\n",
        "      self.norm1 = nn.LayerNorm(d_model)\n",
        "      self.norm2 = nn.LayerNorm(d_model)\n",
        "      self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "      self.dropout1 = nn.Dropout(0.1)\n",
        "      self.dropout2 = nn.Dropout(0.1)\n",
        "      self.dropout3 = nn.Dropout(0.1)\n",
        "\n",
        "      ####################################################################################\n",
        "      # END OF YOUR CODE\n",
        "      ####################################################################################\n",
        "\n",
        "  def forward(self, x, memory, src_mask=None, tgt_mask=None):\n",
        "      \"\"\"\n",
        "      Forward pass for a single decoder layer.\n",
        "\n",
        "      Args:\n",
        "          x (Tensor): Input target sequence of shape (batch_size, target_seq_length, d_model).\n",
        "          memory (Tensor): Encoder output (memory) of shape (batch_size, source_seq_length, d_model).\n",
        "          src_mask (Tensor, optional): Mask for source sequence of shape (batch_size, 1, src_len), defaults to None.\n",
        "          tgt_mask (Tensor, optional): Mask for target sequence of shape (1, 1, target_seq_length), defaults to None.\n",
        "\n",
        "      Returns:\n",
        "          Tensor: Output tensor of shape (batch_size, target_seq_length, d_model) after self-attention, cross-attention, and FFN.\n",
        "      \"\"\"\n",
        "      ####################################################################################\n",
        "      # TODO: Implement the TransformerDecoderLayer forward pass. Refer to the equation\n",
        "      # and image, then fit them into your building blocks.\n",
        "      ####################################################################################\n",
        "\n",
        "      # Your implementation code\n",
        "      self_attention_output = self.self_attention(x, x, x, mask = tgt_mask)\n",
        "      x = x + self.dropout1(self_attention_output)\n",
        "      x = self.norm1(x)\n",
        "\n",
        "      cross_attention_output = self.cross_attention(x, memory, memory, mask = src_mask)\n",
        "      x = x + self.dropout2(cross_attention_output)\n",
        "      x = self.norm2(x)\n",
        "\n",
        "      ff_output = self.feed_forward(x)\n",
        "      x = x + self.dropout3(ff_output)\n",
        "      x = self.norm3(x)\n",
        "\n",
        "      ####################################################################################\n",
        "      # END OF YOUR CODE\n",
        "      ####################################################################################\n",
        "\n",
        "      return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "      \"\"\"\n",
        "      Initializes the Transformer Decoder consisting of multiple decoder layers.\n",
        "\n",
        "      Args:\n",
        "          vocab_size (int): Size of the target vocabulary.\n",
        "          d_model (int): Dimension of the input/output embeddings.\n",
        "          N (int): Number of decoder layers (stacked TransformerDecoderLayers).\n",
        "          num_heads (int): Number of attention heads in each layer's self-attention and cross-attention.\n",
        "          d_ff (int): Dimension of the feed-forward network's hidden layer.\n",
        "\n",
        "      Returns:\n",
        "          None: Sets up the embedding, positional encoding, and N decoder layers.\n",
        "      \"\"\"\n",
        "      def __init__(self, vocab_size, d_model, N, num_heads, d_ff):\n",
        "          super(TransformerDecoder, self).__init__()\n",
        "          ####################################################################################\n",
        "          # TODO: Implement the TransformerDecoder constructor. Think of all building\n",
        "          # blocks you need here. You are able to use `nn.Embedding` and `nn.ModuleList` here,\n",
        "          # and use the provided `PositionalEncoding` class.\n",
        "          ####################################################################################\n",
        "          self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "          self.positional_encoding = PositionalEncoding(d_model)\n",
        "          self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, num_heads, d_ff) for _ in range(N)])\n",
        "          \n",
        "          self.vocab_size = vocab_size\n",
        "\n",
        "      def forward(self, tgt, memory, src_mask=None, tgt_mask=None):\n",
        "          \"\"\"\n",
        "          Forward pass for the full Transformer Decoder.\n",
        "\n",
        "          Args:\n",
        "              tgt (Tensor): Input target sequence tensor of shape (batch_size, target_seq_length).\n",
        "              memory (Tensor): Encoder output (memory) tensor of shape (batch_size, source_seq_length, d_model).\n",
        "              src_mask (Tensor, optional): Mask for the source sequence of shape (batch_size, 1, src_len), defaults to None.\n",
        "              tgt_mask (Tensor, optional): Mask for the target sequence of shape (1, 1, target_seq_length), defaults to None.\n",
        "\n",
        "          Returns:\n",
        "              Tensor: Output tensor of shape (batch_size, target_seq_length, d_model), representing the decoded output.\n",
        "          \"\"\"\n",
        "          ####################################################################################\n",
        "          # TODO: Implement the TransformerDecoder forward pass. Refer to the image to be\n",
        "          # clear. You allowed to use loop here.\n",
        "          ####################################################################################\n",
        "\n",
        "          # Your implementation code\n",
        "          x = self.embedding(tgt)\n",
        "          x = self.positional_encoding(x)\n",
        "          # print(tgt_mask.size())\n",
        "\n",
        "          for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask = src_mask, tgt_mask = tgt_mask)\n",
        "          \n",
        "\n",
        "          ####################################################################################\n",
        "          # END OF YOUR CODE\n",
        "          ####################################################################################\n",
        "\n",
        "          return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBwgqbjSTmCs"
      },
      "source": [
        "### Transformer Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psykBAbMTmCt"
      },
      "source": [
        "The `TransformerTranslator` is your final model which combine both `TransformerEncoder` and `TransformerDecoder` to build a Seq2Seq network. You will use this model to run the translation.\n",
        "\n",
        "<div>\n",
        "        <img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"400\"/>\n",
        "        </div>\n",
        "\n",
        "The translation process works in two modes:\n",
        "- Training Mode (forward): Given the source and target sequences, the model learns to translate by using teacher forcing (i.e., feeding the actual target sequence to the decoder).\n",
        "- Inference Mode (predict): The model generates translations token by token, starting from a special \"beginning-of-sequence\" (BOS) token and stopping when it generates an \"end-of-sequence\" (EOS) token or reaches a maximum length.\n",
        "\n",
        "**Mask**\n",
        "\n",
        "As explained before in the `masked_softmax` function, there are masks that needs to be implemented here. There are two kind of masks:\n",
        "1. Source Mask (Encoder Self-Attention & Decoder Cross-Attention) -> **src_mask**:\n",
        "    - Shape: (B, 1, m), where B is batch size and m is key sequence length\n",
        "    - Used to mask padding tokens in the source sequence\n",
        "    - Prevents attention to padding tokens\n",
        "2. Target Mask (Decoder Self-Attention) -> **tgt_mask**:\n",
        "    - Shape: (1, n, n), where n is query/target sequence length\n",
        "    - Used to prevent attention to future tokens, therefore a token cannot attend to all tokens after it\n",
        "    - Creates causal/autoregressive attention pattern\n",
        "\n",
        "Instructions:\n",
        "- Complete the `TransformerTranslator` init(), forward(), and predict() function. The forward() is used for training therefore use Teacher Forcing, and the predict() is use for inference where tokens are generated autoregressively and become the input to predict next token.\n",
        "- Implemented both `src_mask` and `tgt_mask` and put them as input to the Encoder and Decoder appropriately, in order for the attention to be calculated correctly. Both of these masks are used either in forward() or predict()\n",
        "- Do not need to add Softmax at the end of the forward() or predict(), both function should return the logits of the target vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "VNtoNEFtTmCu",
        "outputId": "bc306629-8c8e-444e-d58e-b829b2e5fe29"
      },
      "outputs": [],
      "source": [
        "class TransformerTranslator(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, N=1, num_heads=4, d_ff=512):\n",
        "      \"\"\"\n",
        "      Initializes the Transformer-based translation model.\n",
        "\n",
        "      Args:\n",
        "          src_vocab_size (int): The size of the source language vocabulary.\n",
        "          tgt_vocab_size (int): The size of the target language vocabulary.\n",
        "          d_model (int, optional): The dimension of the model's embeddings and hidden layers. Defaults to 512.\n",
        "          N (int, optional): The number of encoder and decoder layers. Defaults to 6.\n",
        "          num_heads (int, optional): The number of attention heads in multi-head attention. Defaults to 8.\n",
        "          d_ff (int, optional): The dimension of the feed-forward network's hidden layer. Defaults to 2048.\n",
        "\n",
        "      Returns:\n",
        "          None: Initializes the encoder, decoder, and output layers.\n",
        "      \"\"\"\n",
        "      super(TransformerTranslator, self).__init__()\n",
        "      ####################################################################################\n",
        "      # TODO: Implement the TransformerTranslator constructor. Think of all building\n",
        "      # blocks you need here. You don't need to add softmax here, just until the linear\n",
        "      # output layer, which you should use your own implementation of Linear\n",
        "      ####################################################################################\n",
        "\n",
        "      # Your implementation code\n",
        "      self.encoder = TransformerEncoder(src_vocab_size, d_model, N, num_heads, d_ff)\n",
        "      self.decoder = TransformerDecoder(tgt_vocab_size, d_model, N, num_heads, d_ff)\n",
        "\n",
        "      #self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "      self.output_layer = Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "      self.src_vocab_size = src_vocab_size\n",
        "      self.tgt_vocab_size = tgt_vocab_size\n",
        "\n",
        "      ####################################################################################\n",
        "      # END OF YOUR CODE\n",
        "      ####################################################################################\n",
        "\n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "      \"\"\"\n",
        "      Forward pass for the translation model during training (uses teacher forcing).\n",
        "\n",
        "      Args:\n",
        "          src (Tensor): Source sequence tensor of shape (batch_size, src_seq_len).\n",
        "          src_len (Tensor): Source sequence lengths of shape (batch_size,).\n",
        "          tgt (Tensor): Target sequence tensor of shape (batch_size, tgt_seq_len).\n",
        "          tgt_len (Tensor): Target sequence lengths of shape (batch_size,).\n",
        "\n",
        "      Returns:\n",
        "          Tensor: Logits of shape (batch_size, tgt_seq_len, tgt_vocab_size), representing predicted token probabilities.\n",
        "      \"\"\"\n",
        "      ####################################################################################\n",
        "      # TODO: Implement the TransformerTranslator forward pass. This function will be use\n",
        "      # at training, where target data are provided. Therefore, here we must use teacher\n",
        "      # forcing same with to RNNTranslator. Also, we need to implement the two masks:\n",
        "      # `src_mask` and `tgt_mask` which already explained above. Then, use the `src_mask`\n",
        "      # and the `tgt_mask` accordingly to prevent unwanted attention. Do not need to add\n",
        "      # softmax at the end of the function.\n",
        "      ####################################################################################\n",
        "\n",
        "      # Your implementation code\n",
        "      batch_size = src.size(0)\n",
        "      src_seq_len = src.size(1)\n",
        "      tgt_seq_len = tgt.size(1)\n",
        "\n",
        "    \n",
        "    # (B, 1, seq_len)\n",
        "      src_mask = (src == PAD_TOKEN).unsqueeze(1).to(src.device) # source padding mask\n",
        "      \n",
        "\n",
        "      # Step 2: Compare each position with tgt_len to create mask\n",
        "        # tgt_len: (batch_size,) -> (batch_size, 1)\n",
        "        # seq_range: (1, tgt_seq_len) will broadcast to (batch_size, tgt_seq_len)\n",
        "\n",
        "    # target_causal mask (1, tgt_seq_len, tgt_seq_len)\n",
        "      tgt_mask = torch.triu(torch.ones(tgt_seq_len, tgt_seq_len), diagonal=1).unsqueeze(0).bool().to(src.device)\n",
        "      tgt_padding_mask = (tgt == PAD_TOKEN).unsqueeze(1).bool().to(src.device) # target padding mask\n",
        "      \n",
        "      # tgt_src_mask = tgt_mask |\n",
        "      memory = self.encoder(src, src_mask = src_mask) # (B, seq_len, d_model)\n",
        "      out = self.decoder(tgt, memory, src_mask = src_mask, tgt_mask = tgt_mask | tgt_padding_mask) # (B, tgt_seq_len, d_model)\n",
        "      output = self.output_layer(out)\n",
        "      \n",
        "      \n",
        "      # eos_score = torch.zeros((self.tgt_vocab_size)).to(src.device)\n",
        "      # eos_score[EOS_TOKEN]\n",
        "      # output[:, -1, :] = eos_score\n",
        "      # output[:, 0, :] = bos_score\n",
        "      # print(output[0, -1, :])\n",
        "\n",
        "      ####################################################################################\n",
        "      # END OF YOUR CODE\n",
        "      ####################################################################################\n",
        "\n",
        "      return output\n",
        "\n",
        "  def predict(self, src, src_len):\n",
        "    batch_size = src.shape[0]\n",
        "    device = src.device\n",
        "\n",
        "    src_seq_len = src.size(1)\n",
        "    src_mask = (src == PAD_TOKEN).unsqueeze(1).to(src.device)\n",
        "    memory = self.encoder(src, src_mask=src_mask)\n",
        "\n",
        "    # 시작 토큰으로 초기화\n",
        "    tgt = torch.full((batch_size, 1), BOS_TOKEN, dtype=torch.long, device=device)\n",
        "    # tgt = torch.cat([tgt, torch.full((batch_size, 1), 1234, dtype=torch.long, device=device)], dim=1)\n",
        "\n",
        "    for _ in range(MAX_LEN):\n",
        "        # 현재까지 생성된 토큰들로 tgt_input 구성\n",
        "        tgt_input = tgt\n",
        "        # 마스크 생성\n",
        "        tgt_mask = torch.triu(torch.ones(tgt_input.size(1), tgt_input.size(1), device=device), diagonal=1).bool()\n",
        "        # 디코더 실행\n",
        "        out = self.decoder(tgt_input, memory, src_mask=src_mask, tgt_mask=tgt_mask.unsqueeze(0))\n",
        "        logits = self.output_layer(out)\n",
        "        # 마지막 토큰의 출력만 사용하여 다음 토큰 예측\n",
        "        next_token_logits = logits[:, -1, :]\n",
        "        next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "        # 다음 토큰을 tgt에 추가\n",
        "        tgt = torch.cat([tgt, next_token.unsqueeze(1)], dim=1)\n",
        "        # print(tgt[0,:])\n",
        "        # 모든 시퀀스가 EOS를 생성했는지 확인\n",
        "        if (next_token == EOS_TOKEN).all():\n",
        "            break\n",
        "    \n",
        "    # TODO: tgt matrix should have shape of (B, MAX_LEN + 2). add PAD tokens to make it so.\n",
        "    if tgt.size(1) < MAX_LEN + 2:\n",
        "        pad = torch.full((batch_size, MAX_LEN + 2 - tgt.size(1)), PAD_TOKEN, dtype=torch.long, device=device)\n",
        "        tgt = torch.cat([tgt, pad], dim=1)\n",
        "\n",
        "\n",
        "    # 첫 번째 EOS 이후의 토큰을 PAD로 대체\n",
        "    for i in range(batch_size):\n",
        "        eos_positions = (tgt[i] == EOS_TOKEN).nonzero(as_tuple=True)[0]\n",
        "        if eos_positions.numel() > 0:\n",
        "            first_eos_idx = eos_positions[0]\n",
        "            tgt[i, first_eos_idx + 1:] = PAD_TOKEN\n",
        "        else:\n",
        "            tgt[i, :] = PAD_TOKEN\n",
        "    \n",
        "    return tgt\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dq5VUhwTmCv"
      },
      "source": [
        "## TransformerTranslator Training & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkkyYvplTmCw"
      },
      "source": [
        "Below we will initialize, train, and evaluate your implemented `TransformerTranslator`. If you coded all the training and evaluation function properly, you should be able to re-use those function for the `TransformerTranslator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "HvYKkwyDTmCw"
      },
      "outputs": [],
      "source": [
        "transformer_net = TransformerTranslator(eng_vocab.vocab_size, fra_vocab.vocab_size, N = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N8V4kaYTmCx"
      },
      "outputs": [],
      "source": [
        "trained_transformer_net = train_model(transformer_net, train_iter, val_iter, lr, 10, DEVICE, run_name=\"Transformer-Baseline\")\n",
        "acc = evaluate_model(trained_transformer_net, val_iter, DEVICE)\n",
        "print(acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XSmx2wUTmCy"
      },
      "outputs": [],
      "source": [
        "acc = evaluate_model(trained_transformer_net, val_iter, DEVICE)\n",
        "print(acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmGpQaeuTmCz"
      },
      "source": [
        "### Exploring TransformerTranslator Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nZAlLn4TmC0"
      },
      "source": [
        "There are a lot of things to tune on the `TransformerTranslator`. Therefore, do some little experiments to find the optimal hyperparameters for `TransformerTranslator`. The dataset that we used is considered small, with really short length. Maybe the excessive number of params could lead to degradation of performance.\n",
        "\n",
        "Instructions:\n",
        "- Explore different parameters of `TransformerTranslator`. Do training and evaluation on it, but do not run it on `test_iter`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "EpaXvqJjTmC1"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE BELOW\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-6L6wDPTmC2"
      },
      "source": [
        "## Your Own RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl2KtcAITmC3"
      },
      "source": [
        "As you can see from the RNN variants (also attention has some variants but not implemented here), for the same idea/mechanism you can have multiple algorithm on how to implement it. Now, use your creativity to define your own RNN algorithm. Then, use them in the RNNTranslator and see whether they are better than the 3 variants you have implemented.\n",
        "\n",
        "Instructions:\n",
        "- Implement the `MyRNNCell` and `MyRNN` class according to your idea of your RNN.\n",
        "- Explain your idea and implementation\n",
        "- Try train and evaluate your `MyRNN` using the `RNNTranslator`. Make sure it is compatible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "s9-WhZoITmC4"
      },
      "outputs": [],
      "source": [
        "class MyRNNCell(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size):\n",
        "            pass\n",
        "\n",
        "        def forward(self):\n",
        "            pass\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size):\n",
        "            pass\n",
        "\n",
        "        def forward(self,):\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsQHPB7bTmC5"
      },
      "source": [
        "TODO: Explain your idea and implementation of your RNN here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9PnAvcS-TmC7"
      },
      "outputs": [],
      "source": [
        "## Your code to train MyRNN using RNNTranslator\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foQNXcn4TmC7"
      },
      "source": [
        "## Test Data Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xIgIDmlTmC8"
      },
      "source": [
        "Your challenge is to experiment with different model architectures, hyperparameters, and training strategies to find the best-performing model on the validation data. Once you have identified your best model, you will then run it on the test data only once at the end of the process. Keep track of your experiments and results carefully, as the test run should only be performed once after you're found your best model. confident in your model's performance.\n",
        "\n",
        "We will make a leaderboard for this and thus provide a bonus point for the top-5 performers.\n",
        "\n",
        "Instructions:\n",
        "- Feel free to experiment in order to find the best model.\n",
        "- You can only use all building blocks that have been implemented in this code, no other code allowed from outside such as using pytorch built-in `nn.RNN` class\n",
        "- The best model architecture and hyperparameters should be made clear, and thus save it on the variable `best_model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8-xrC4x1TmC9"
      },
      "outputs": [],
      "source": [
        "### Your code to experiment finding the best model\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJgvvvrnTmC_"
      },
      "outputs": [],
      "source": [
        "best_acc = evaluate_model(best_model, test_iter, DEVICE)\n",
        "print(best_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
